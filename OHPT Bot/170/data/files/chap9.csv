ChapterâŒ˜PageâŒ˜Text
9âŒ˜0âŒ˜Chapter 9 Coping with NP-completeness Youarethejunior member ofaseasoned project team. Yourcurrent task istowrite code for solving asimple-looking problem involving graphs andnumbers .What areyousupposed to do? Ifyouarevery lucky,your problem will beamong thehalf-dozen problems concerning graphs with weights (shortest path, minimum spanning tree,maximum ow,etc.),that we havesolved inthis book. Even ifthis isthecase,recognizing suchaproblem initsnatural habitatÂ—grungy andobscured byreality andcontextÂ—requires practice andskill. Itismore likely that youwillneed toreduce your problem tooneofthese luckyonesÂ—or tosolve itusing dynamic programming orlinear programming . Butchances arethat nothing likethiswillhappen. Theworld ofsearc hproblems isableak landscape .There areafewspots oflightÂ—brilliant algorithmic ideasÂ—eac hilluminating a small area around it(the problems that reduce toit;twoofthese areas ,linear anddynamic programming ,areinfactdecently large). Buttheremaining vast expanse ispitchdark: NP- complete .What areyoutodo? Youcanstart byproving that your problem isactually NP-complete .Often aproof by generalization (recall thediscussion onpage 270andExercise 8.10) isallthat youneed; and sometimes asimple reduction from 3SATorZOEisnottoodifcult tond. This sounds likea theoretical exercise ,but, ifcarried outsuccessfully ,itdoes bring some tangible rewards: now your status intheteam hasbeen elevated, youarenolonger thekidwho can't do,and you havebecome thenoble knight with theimpossible quest. But, unfortunately ,aproblem does notgoawaywhen proved NP-complete .Therealques- tion is,Whatdoyoudonext? This isthesubject ofthepresent chapter and also theinspiration forsome ofthemost important modern researc honalgorithms and complexity .NP-completeness isnotadeath certicateÂ—it isonly thebeginning ofafascinating adventure . Yourproblem' sNP-completeness proof probably constructs graphs that arecomplicated and weird, very muchunlike those that come upinyour application. Forexample ,even though SATisNP-complete ,satisfying assignments forHORNSAT(the instances ofSATthat come upinlogic programming) canbefound efciently (recall Section 5.3). Or,suppose the graphs that arise inyour application aretrees .Inthis case,many NP-complete problems , 283
9âŒ˜1âŒ˜284 Algorithms suchasINDEPENDENTSET,canbesolved inlinear time bydynamic programming (recall Section 6.7). Unfortunately ,this approac hdoes notalwayswork. Forexample ,weknow that 3SAT isNP-complete .And the INDEPENDENTSETproblem, along with many other NP-complete problems ,remains soeven forplanar graphs (graphs that canbedrawnintheplane without crossing edges). Moreover ,often youcannot neatly characterize theinstances that come up inyour application. Instead, youwill havetorely onsome form ofintelligent exponential searc hÂ—procedures suchasbacktrac king andbranc hand bound whic hareexponential time intheworst-case ,but, with theright design, could bevery efcient ontypical instances that come upinyour application. Wediscuss these methods inSection 9.1. Oryoucandevelop analgorithm foryour NP-complete optimization problem that falls short oftheoptimum butnever bytoomuch.Forexample ,inSection 5.4wesawthat the greedy algorithm alwaysproduces asetcover that isnomore than logntimes theoptimal setcover .Analgorithm that achieves suchaguarantee iscalled anapproximation algorithm. AswewillseeinSection 9.2,suchalgorithms areknown formany NP-complete optimization problems ,andthey aresome ofthemost clever andsophisticated algorithms around. And the theory ofNP-completeness canagain beused asaguide inthisendea vor,byshowing that, for some problems ,there areeven severe limits tohow well they canbeapproximatedÂ—unless of course P=NP. Finally ,there areheuristics ,algorithms with noguarantees oneither therunning time or thedegree ofapproximation. Heuristics rely oningenuity ,intuition, agood understanding oftheapplication, meticulous experimentation, andoften insights from physics orbiology ,to attac kaproblem. Weseesome common kinds inSection 9.3. 9.1 Intelligent exhaustive search 9.1.1 Backtracking Backtrac king isbased ontheobservation that itisoften possible toreject asolution bylooking atjustasmall portion ofit.Forexample ,ifaninstance ofSATcontains theclause (x1_x2), then allassignments withx1=x2=0(i.e.,false )canbeinstantly eliminated. Toput itdifferently ,byquicklychecking and discrediting this partial assignment ,weareable to prune aquarter oftheentire searc hspace .Apromising direction, butcanitbesystematically exploited? Here' show itisdone .Consider theBoolean formula(w;x;y;z)specied bythesetof clauses (w_x_y_z);(w_x);(x_y);(y_z);(z_w);(w_z): Wewill incrementally grow atree ofpartial solutions .Westart bybranc hing onany one variable ,sayw:
9âŒ˜2âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 285 Initial formula  w=1 w=0 Pluggingw=0andw=1into,wend that noclause isimmediately violated and thus neither ofthese twopartial assignments canbeeliminated outright. Soweneed tokeep branc hing.Wecanexpand either ofthetwoavailable nodes ,andonanyvariable ofourchoice . Let'strythisone: Initial formula  w=1 w=0 x=0 x=1 This time,weareinluck.Thepartial assignment w=0;x=1violates theclause (w_x) andcanbeterminated, thereby pruning agood chunk ofthesearc hspace .Webacktrac kout ofthiscul-de-sac andcontinue ourexplorations atoneofthetworemaining active nodes . Inthismanner ,backtrac king explores thespace ofassignments ,growing thetree only at nodes where there isuncertainty about theoutcome ,andstopping ifatanystage asatisfying assignment isencountered. Inthecase ofBoolean satisability ,eachnode ofthesearc htree canbedescribed either byapartial assignment orbytheclauses that remain when those values areplugged intothe original formula. Forinstance ,ifw=0andx=0then anyclause withworxisinstantly satised andanyliteralworxisnotsatised andcanberemoved. What' sleftis (y_z);(y);(y_z): Likewise ,w=0andx=1leaves ();(y_z); with theÂ“empty clauseÂ” ()ruling outsatisability .Thus thenodes ofthesearc htree,repre- senting partial assignments ,arethemselves SATsubproblems . This alternative representation ishelpful formaking thetwodecisions that repeatedly arise: whic hsubproblem toexpand next, andwhic hbranc hing variable touse.Since theben- etofbacktrac king liesinitsability toeliminate portions ofthesearc hspace ,andsince this happens only when anempty clause isencountered, itmakes sense tochoose thesubproblem that contains thesmallest clause andtothen branc honavariable inthat clause .Ifthisclause
9âŒ˜3âŒ˜286 Algorithms Figure 9.1Backtrac king reveals thatisnotsatisable . ();(y_z) (y_z);(y);(y_z) (z);(z)(x_y);(y_z);(z);(z) (x_y);(y);() (x_y);()(w_x_y_z);(w_x);(x_y);(y_z);(z_w);(w_z) (x_y_z);(x);(x_y);(y_z) x=1 ()z=0 z=1 ()()y=1z=1 z=0 y=0w=1 w=0 x=0 happens tobeasingleton, then atleast oneoftheresulting branc heswillbeterminated. (If there isatieinchoosing subproblems ,onereasonable policy istopicktheonelowest inthe tree,inthehope that itisclosetoasatisfying assignment.) SeeFigure 9.1fortheconclusion ofourearlier example . More abstractly ,abacktrac king algorithm requires atestthat looks atasubproblem and quicklydeclares oneofthree outcomes: 1.Failure: thesubproblem hasnosolution. 2.Success: asolution tothesubproblem isfound. 3.Uncertainty . Inthecase ofSAT,this testdeclares failure ifthere isanempty clause ,success ifthere are noclauses ,and uncertainty otherwise .The backtrac king procedure then hasthefollowing format. StartwithsomeproblemP0 LetS=fP0g,thesetofactivesubproblems RepeatwhileSisnonempty: chooseasubproblem P2SandremoveitfromS expanditintosmaller subproblems P1;P2;:::;Pk ForeachPi: Iftest(Pi)succeeds: haltandannounce thissolution Iftest(Pi)fails: discardPi
9âŒ˜4âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 287 Otherwise: addPitoS Announce thatthereisnosolution ForSAT,thechoose procedure picksaclause ,andexpand picksavariable within that clause . Wehavealready discussed some reasonable waysofmaking suchchoices . Withtherighttest ,expand ,andchoose routines ,backtrac king canberemarkably effec- tiveinpractice .Thebacktrac king algorithm weshowed forSATisthebasis ofmany successful satisability programs .Another sign ofquality isthis: ifpresented with a2SATinstance ,it willalwaysndasatisfying assignment, ifoneexists ,inpolynomial time (Exercise 9.1)! 9.1.2 Branch-and-bound The same principle canbegeneralized from searc hproblems suchasSATtooptimization problems .Forconcreteness ,let'ssaywehaveaminimization problem; maximization will follow thesame pattern. Asbefore ,wewill deal with partial solutions ,eachofwhic hrepresents asubproblem , namely ,what isthe(cost ofthe) best waytocomplete this solution? And asbefore ,weneed abasis foreliminating partial solutions ,since there isnoother source ofefciency inour method. Toreject asubproblem, wemust becertain that itscost exceeds that ofsome other solution wehavealready encountered. Butitsexact cost isunknown tousandisgenerally notefciently computable .Soinstead weuseaquicklower bound onthiscost. StartwithsomeproblemP0 LetS=fP0g,thesetofactivesubproblems bestsofar =1 RepeatwhileSisnonempty: chooseasubproblem (partial solution) P2SandremoveitfromS expanditintosmaller subproblems P1;P2;:::;Pk ForeachPi: IfPiisacomplete solution: updatebestsofar elseiflowerbound (Pi)<bestsofar :addPitoS returnbestsofar Let'sseehow this works forthetraveling salesman problem onagraphG=(V;E)with edge lengthsde>0.Apartial solution isasimple patha bpassing through some vertices SV,whereSincludes theendpointsaandb.Wecandenote suchapartial solution bythe tuple [a;S;b]Â—in fact,awillbexed throughout thealgorithm. Thecorresponding subproblem istondthebest completion ofthetour,that is,thecheapest complementary pathb awith intermediate nodesVS.Notice that theinitial problem isoftheform[a;fag;a]foranya2V ofourchoosing . Ateachstep ofthebranc h-and-bound algorithm, weextend aparticular partial solution [a;S;b]byasingle edge(b;x),wherex2VS.There canbeuptojVSjwaystodothis,and eachofthese branc hesleads toasubproblem oftheform[a;S[fxg;x].
9âŒ˜5âŒ˜288 Algorithms How canwelower -bound thecostofcompleting apartial tour[a;S;b]?Many sophisticated methods havebeen developed forthis,butlet'slook atarather simple one.Theremainder of thetour consists ofapath throughVS,plus edges fromaandbtoVS.Therefore ,itscost isatleast thesum ofthefollowing: 1.Thelightest edge fromatoVS. 2.Thelightest edge frombtoVS. 3.Theminimum spanning tree ofVS. (Doyouseewhy?) And this lower bound canbecomputed quicklybyaminimum spanning treealgorithm. Figure 9.2runs through anexample: eachnode ofthetreerepresents apartial tour (specically ,thepath from theroot tothat node) that atsome stage isconsidered bythe branc h-and-bound procedure .Notice how just28partial solutions areconsidered, instead of the7!=5;040that would arise inabrute-force searc h.
9âŒ˜6âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 289 Figure 9.2(a)Agraph and itsoptimal traveling salesman tour.(b)The branc h-and-bound searc htree,explored lefttoright. Boxed numbers indicate lower bounds oncost. (a) A BCDE F G H 1 211 121 2 51 11 A BCDE F G H 1 11 11 1 11 (b) A EH F GB F GD 15 148 B D C D H G H8E C G inf8 10 13 12 88 14 888 810 C10 G E F G HD 111111 11infH G 1414 10 10 Cost: 11 Cost: 8
9âŒ˜7âŒ˜290 Algorithms 9.2 Approximation algorithms Inanoptimization problem wearegiven aninstanceIand areasked tond theoptimum solutionÂ—the onewith themaximum gain ifwehaveamaximization problem like INDEPEN- DENTSET,ortheminimum cost ifwearedealing with aminimization problem suchasthe TSP.Forevery instanceI,letusdenote byOPT(I)thevalue (benet orcost) oftheoptimum solution. Itmakes themath alittle simpler (and isnottoofarfrom thetruth) toassume that OPT(I)isalways apositive integer . Wehavealready seen anexample ofa(famous) approximation algorithm inSection 5.4: thegreedy scheme forSETCOVER.ForanyinstanceIofsizen,weshowed that this greedy algorithm isguaranteed toquicklynd asetcover ofcardinality atmost OPT(I)logn.This lognfactor isknown astheapproximation guarantee ofthealgorithm. More generally ,consider anyminimization problem. Suppose now that wehaveanalgo- rithmAforourproblem whic h,given aninstanceI,returns asolution with valueA(I).The approximation ratio ofalgorithmAisdened tobe A=max IA(I) OPT(I): Inother words , Ameasures bythefactor bywhic htheoutput ofalgorithmAexceeds the optimal solution, ontheworst-case input. The approximation ratio canalso bedened for maximization problems ,suchasINDEPENDENTSET,inthesame wayÂ—except that togeta number larger than1wetake thereciprocal. So,when faced with anNP-complete optimization problem, areasonable goal istolook for anapproximation algorithmAwhose Aisassmall aspossible .Butthis kind ofguarantee might seem alittle puzzling: How canwecome close totheoptimum ifwecannot determine theoptimum? Let'slook atasimple example . 9.2.1 Vertex cover Wealready know the VERTEXCOVERproblem isNP-hard. VERTEXCOVER Input: Anundirected graphG=(V;E). Output: Asubset oftheverticesSVthat touchesevery edge. Goal: MinimizejSj. SeeFigure 9.3foranexample . Since VERTEXCOVERisaspecial case ofSETCOVER,weknow from Chapter 5that itcan beapproximated within afactor ofO(logn)bythegreedy algorithm: repeatedly delete the vertex ofhighest degree andinclude itinthevertex cover .And there aregraphs onwhic hthe greedy algorithm returns avertex cover that isindeed logntimes theoptimum. Abetter approximation algorithm forVERTEXCOVERisbased onthenotion ofamatc hing , asubset ofedges that havenovertices incommon (Figure 9.4). Amatc hing ismaximal ifno
9âŒ˜8âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 291 Figure 9.3Agraph whose optimal vertex cover ,shown shaded, isofsize8. Figure 9.4(a)Amatc hing,(b)itscompletion toamaximal matc hing,and (c)theresulting vertex cover . (a) (b) (c) more edges canbeadded toit.Maximal matc hings willhelp usnd good vertex covers ,and moreover ,they areeasy togenerate: repeatedly pickedges that aredisjoint from theones chosen already ,until thisisnolonger possible . What istherelationship between matc hings and vertex covers? Here isthecrucial fact: anyvertex cover ofagraphGmust beatleast aslarge asthenumber ofedges inanymatc hing inG;that is,anymatc hing provides alower bound onOPT.This issimply because eachedge ofthematc hing must becovered byoneofitsendpoints inanyvertex cover! Finding sucha lower bound isakeystep indesigning anapproximation algorithm, because wemust compare thequality ofthesolution found byouralgorithm toOPT,whic hisNP-complete tocompute . One more observation completes thedesign ofourapproximation algorithm: letSbea setthat contains both endpoints ofeachedge inamaximal matc hingM.ThenSmust bea vertex cover Â—ifitisn't, that is,ifitdoesn't touchsome edgee2E,thenMcould notpossibly bemaximal since wecould still addetoit.ButourcoverShas2jMjvertices .And from the previous paragraph weknow that anyvertex cover must havesizeatleastjMj.Sowe're done . Here' sthealgorithm forVERTEXCOVER. Findamaximal matching ME
9âŒ˜9âŒ˜292 Algorithms ReturnS=fallendpoints ofedgesinMg This simple procedure alwaysreturns avertex cover whose sizeisatmost twice optimal! Insummary ,even though wehavenowayofnding thebest vertex cover ,wecaneasily ndanother structure ,amaximal matc hing,with twokeyproperties: 1.Itssizegives usalower bound ontheoptimal vertex cover . 2.Itcanbeused tobuild avertex cover ,whose size canberelated tothat oftheoptimal cover using property 1. Thus ,thissimple algorithm hasanapproximation ratio of A2.Infact, itisnothard to ndexamples onwhic hitdoes make a100% error; hence A=2. 9.2.2 Clustering Weturn next toaclustering problem, inwhic hwehavesome data (text documents ,say,or images ,orspeec hsamples) that wewanttodivide intogroups .Itisoften useful todene Â“dis- tancesÂ” between these data points ,numbers that capture how close orfarthey arefrom one another .Often thedata aretrue points insome high-dimensional space andthedistances are theusual Euclidean distance; inother cases ,thedistances aretheresult ofsome Â“similarity testsÂ” towhic hwehavesubjected thedata points .Assume that wehavesuchdistances and that they satisfy theusual metric properties: 1.d(x;y)0forallx;y. 2.d(x;y)=0ifandonly ifx=y. 3.d(x;y)=d(y;x). 4.(Triangle inequality) d(x;y)d(x;z)+d(z;y). Wewould liketopartition thedata points intogroups that arecompact inthesense ofhaving small diameter . k-CLUSTER Input: PointsX=fx1;:::;xngwith underlying distance metricd(;);integerk. Output: Apartition ofthepoints intokclustersC1;:::;Ck. Goal: Minimize thediameter oftheclusters , max jmax xa;xb2Cjd(xa;xb): One waytovisualize this task istoimaginenpoints inspace ,whic haretobecovered byk spheres ofequal size.What isthesmallest possible diameter ofthespheres? Figure 9.5shows anexample . This problem isNP-hard, buthasavery simple approximation algorithm. The idea isto pickkofthedata points ascluster center sandtothen assign eachoftheremaining points to
9âŒ˜10âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 293 Figure 9.5Some data points andtheoptimalk=4clusters . Figure 9.6(a)Fourcenters chosen byfarthest-rst traversal. (b)Theresulting clusters . (a)2 14 3(b) thecenter closest toit,thus creatingkclusters .The centers arepickedoneatatime,using anintuitive rule: alwayspickthenext center tobeasfaraspossible from thecenters chosen sofar(seeFigure 9.6). Pickanypoint12Xasthefirstcluster center fori=2tok: LetibethepointinXthatisfarthest from1;:::;i1 (i.e.,thatmaximizes minj<id(;j)) Createkclusters: Ci=fallx2Xwhoseclosest centerisig It'sclear that thisalgorithm returns avalid partition. What' smore ,theresulting diameter is guaranteed tobeatmost twice optimal. Here' stheargument. Letx2Xbethepoint farthest from1;:::;k(inother words the next center wewould havechosen, ifwewantedk+1ofthem), andletrbeitsdistance toits closest center .Then every point inXmust bewithin distancerofitscluster center .Bythe triangle inequality ,thismeans that every cluster hasdiameter atmost 2r. Buthow doesrrelate tothediameter oftheoptimal clustering? Well,wehaveidentied k+1pointsf1;2;:::;k;xgthat areallatadistance atleastrfrom eachother (why?). Any partition intokclusters must puttwoofthese points inthesame cluster andmust therefore
9âŒ˜11âŒ˜294 Algorithms havediameter atleastr. This algorithm hasacertain high-level similarity toourscheme forVERTEXCOVER.In- stead ofamaximal matc hing,weuseadifferent easily computable structureÂ—a setofkpoints that cover allofXwithin some radiusr,while atthesame time being mutually separated byadistance ofatleastr.This structure isused both togenerate aclustering andtogive a lower bound ontheoptimal clustering . Weknow ofnobetter approximation algorithm forthisproblem. 9.2.3 TSP Thetriangle inequality playedacrucial roleinmaking thek-CLUSTERproblem approximable . Italsohelps with the TRAVELINGSALESMANPROBLEM:ifthedistances between cities satisfy themetric properties ,then there isanalgorithm that outputs atour oflength atmost 1:5 times optimal. We'llnow look ataslightly weaker result that achieves afactor of2. Continuing with thethought processes ofourprevious twoapproximation algorithms ,we canaskwhether there issome structure that iseasy tocompute andthat isplausibly related tothebest traveling salesman tour (aswell asproviding agood lower bound onOPT).Alittle thought andexperimentation reveals theanswer tobetheminimum spanning tree. Let'sunderstand thisrelation. Removing anyedge from atraveling salesman tour leaves apath through allthevertices ,whic hisaspanning tree.Therefore , TSPcostcostofthispathMSTcost: Now,wesomehow need tousetheMST tobuild atraveling salesman tour.Ifwecanuseeach edge twice ,then byfollowing theshape oftheMST weendupwith atour that visits allthe cities ,some ofthem more than once.Here' sanexample ,with theMST ontheleftand the resulting tour ontheright (the numbers show theorder inwhic htheedges aretaken). TulsaAlbuquerqueAmarilloWichita Little Rock Dallas Houston San AntonioEl PasoTulsaWichita Little Rock Dallas HoustonEl PasoAmarillo San AntonioAlbuquerque5 2 110 91187 126 4 13 14 3 15 16 Therefore ,this tour hasalength atmost twice theMST cost, whic haswe've already seen is atmost twice theTSP cost. This istheresult wewanted, butwearen't quite done because ourtour visits some cities multiple times andistherefore notlegal. Toxtheproblem, thetour should simply skip any cityitisabout torevisit, andinstead move directly tothenext new cityinitslist:
9âŒ˜12âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 295 TulsaWichita Little Rock Dallas Houston San AntonioEl PasoAlbuquerque Amarillo Bythetriangle inequality ,these bypasses canonly make theoverall tour shorter . General TSP Butwhat ifweareinterested ininstances ofTSPthat donotsatisfy thetriangle inequality? Itturns outthat thisisamuchharder problem toapproximate . Here iswhy: Recall that onpage 274wegaveapolynomial-time reduction whic hgiven anygraphGandinteger anyC>0produces aninstanceI(G;C)ofthe TSPsuchthat: (i)IfGhasaRudrata path, then OPT(I(G;C))=n,thenumber ofvertices inG. (ii)IfGhasnoRudrata path, then OPT(I(G;C))n+C. This means that even anapproximate solution toTSPwould enable ustosolve RUDRATA PATH!Let'swork outthedetails . Consider anapproximation algorithmAforTSPandlet Adenote itsapproximation ratio . From anyinstanceGofRUDRATAPATH,wewillcreate aninstanceI(G;C)ofTSPusing the specic constantC=n A.What happens when algorithmAisrunonthis TSPinstance? In case (i),itmust output atour oflength atmost AOPT(I(G;C))=n A,whereas incase (ii)it must output atour oflength atleast OPT(I(G;C))>n A.Thus wecangure outwhetherG hasaRudrata path! Here istheresulting procedure: GivenanygraphG: computeI(G;C)(withC=n A)andrunalgorithmAonit iftheresulting tourhaslengthn A: conclude thatGhasaRudrata path else:conclude thatGhasnoRudrata path This tells uswhether ornotGhasaRudrata path; bycalling theprocedure apolynomial number oftimes ,wecanndtheactual path (Exercise 8.2). We've shown that ifTSPhasapolynomial-time approximation algorithm, then there is apolynomial algorithm fortheNP-complete RUDRATAPATHproblem. So,unless P=NP, there cannot exist anefcient approximation algorithm forthe TSP.
9âŒ˜13âŒ˜296 Algorithms 9.2.4 Knapsack Our lastapproximation algorithm isforamaximization problem andhasavery impressive guarantee: given any>0,itwillreturn asolution ofvalue atleast (1)times theoptimal value ,intime that scales only polynomially intheinput sizeandin1=. The problem isKNAPSACK,whic hwerst encountered inChapter 6.There arenitems , with weightsw1;:::;wnandvaluesv1;:::;vn(allpositive integers), andthegoal istopickthe most valuable combination ofitems subject totheconstraint that their total weight isatmost W. Earlier wesawadynamic programming solution tothisproblem with running timeO(nW). Using asimilar technique ,arunning time ofO(nV)canalso beachieved, whereVisthesum ofthevalues .Neither ofthese running times ispolynomial, becauseWandVcanbevery large ,exponential inthesizeoftheinput. Let'sconsider theO(nV)algorithm. Inthebadcase whenVislarge ,what ifwesimply scale down allthevalues insome way?Forinstance ,if v1=117;586;003;v2=738;493;291;v3=238;827;453; wecould simply knoc koffsome precision and instead use117,738,and238.This doesn't change theproblem allthat muchandwillmake thealgorithm much,muchfaster! Now forthedetails .Along with theinput, theuser isassumed tohavespecied some approximation factor>0. Discard anyitemwithweight>W Letvmax=max ivi Rescale valuesbvi=bvin vmaxc Runthedynamic programming algorithm withvaluesfbvig Outputtheresulting choiceofitems Let'sseewhy this works .Firstofall,since therescaled valuesbviareallatmostn=,the dynamic program isefcient, running intimeO(n3=). Now suppose theoptimal solution totheoriginal problem istopicksome subset ofitems S,with total valueK.Therescaled value ofthissame assignment is X i2Sbvi=X i2S vin vmax X i2S vin vmax1 Kn vmaxn: Therefore ,theoptimal assignment fortheshrunken problem, callitbS,hasarescaled value of atleast thismuch.Interms oftheoriginal values ,assignmentbShasavalue ofatleast X i2bSviX i2bSbvivmax n Kn vmaxn vmax n=KvmaxK(1):
9âŒ˜14âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 297 9.2.5 The approximability hierarchy Given any NP-complete optimization problem, weseek thebest approximation algorithm possible .Failing this,wetrytoprove lower bounds ontheapproximation ratios that are achievable inpolynomial time (wejustcarried outsuchaproof forthegeneral TSP). Alltold, NP-complete optimization problems areclassied asfollows: Those forwhic h,liketheTSP,nonite approximation ratio ispossible . Those forwhic hanapproximation ratio ispossible ,butthere arelimits tohow small this canbe.VERTEXCOVER,k-CLUSTER,andtheTSP with triangle inequality belong here.(Forthese problems wehavenotestablished limits totheir approximability ,but these limits doexist, andtheir proofs constitute some ofthemost sophisticated results inthiseld.) Down below wehaveamore fortunate class ofNP-complete problems forwhic hap- proximability hasnolimits ,andpolynomial approximation algorithms with error ratios arbitrarily closetozero exist. KNAPSACKresides here. Finally ,there isanother class ofproblems ,between therst twogiven here,forwhic h theapproximation ratio isabout logn.SETCOVERisanexample . (Ahumbling reminder: Allthis iscontingent upon theassumption P6=NP.Failing this, thishierarc hycollapses down toP,andallNP-complete optimization problems canbesolved exactly inpolynomial time.) Anal point onapproximation algorithms: often these algorithms ,ortheir variants ,per- form muchbetter ontypical instances than their worst-case approximation ratio would have youbelieve . 9.3 Local search heuristics Our next strategy forcoping with NP-completeness isinspired byevolution (whic his,after all,theworld' sbest-tested optimization procedure)Â—by itsincremental process ofintroducing small mutations ,trying them out, and keeping them ifthey work well. This paradigm is called local searc hand canbeapplied toany optimization task. Here' show itlooks fora minimization problem. letsbeanyinitial solution whilethereissomesolution s0intheneighborhood ofs forwhichcost(s0)<cost(s):replacesbys0 returns Oneachiteration, thecurrent solution isreplaced byabetter oneclose toit,initsneigh- borhood .This neighborhood structure issomething weimpose upon theproblem andisthe central design decision inlocal searc h.Asanillustration, let'srevisit thetraveling salesman problem.
9âŒ˜15âŒ˜298 Algorithms 9.3.1 Traveling salesman, once more Assume wehaveallinterpoint distances betweenncities ,giving asearc hspace of(n1)! different tours .What isagood notion ofneighborhood? The most obvious notion istoconsider twotours asbeing close ifthey differ injustafew edges .They can't differ injustoneedge (doyouseewhy?), sowewillconsider differences of twoedges .Wedene the2-change neighborhood oftoursasbeing thesetoftours that canbe obtained byremoving twoedges ofsandthen putting intwoother edges .Here' sanexample ofalocal move: Wenow haveawell-dened local searc hprocedure .How does itmeasure upunder ourtwo standard criteria foralgorithmsÂ—what isitsoverall running time,anddoes italwaysreturn thebest solution? Embarrassingly ,neither ofthese questions hasasatisfactory answer .Eachiteration is certainly fast, because atour hasonlyO(n2)neighbors .However ,itisnotclear how many iterations will beneeded: whether forinstance ,there might beanexponential number of them. Likewise ,allwecaneasily sayabout thenal tour isthat itislocall yoptimal Â—that is,itissuperior tothetours initsimmediate neighborhood. There might bebetter solutions further away.Forinstance ,thefollowing picture shows apossible nal answer that isclearly suboptimal; therange oflocal moves issimply toolimited toimprove upon it. Toovercome this,wemaytryamore generous neighborhood, forinstance 3-change ,con- sisting oftours that differ onuptothree edges .And indeed, thepreceding bad case gets xed: Butthere isadownside ,inthat thesize ofaneighborhood becomesO(n3),making each iteration more expensive .Moreover ,there maystill besuboptimal local minima, although fewer than before .Toavoid these ,wewould havetogoupto4-change ,orhigher .Inthis manner ,efciency andquality often turn outtobecompeting considerations inalocal searc h. Efciency demands neighborhoods that canbesearc hedquickly,butsmaller neighborhoods
9âŒ˜16âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 299 canincrease theabundance oflow-quality local optima. Theappropriate compromise istypi- cally determined byexperimentation.
9âŒ˜17âŒ˜300 Algorithms Figure 9.7(a)Nine American cities .(b)Local searc h,starting atarandom tour,andusing 3-change .Thetraveling salesman tour isfound after three moves . (a) TulsaAlbuquerqueAmarilloWichita Little Rock Dallas Houston San AntonioEl Paso (b) (i) (ii) (iii) (iv)
9âŒ˜18âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 301 Figure 9.8Local searc h. Figure 9.7shows aspecic example oflocal searc hatwork. Figure 9.8isamore abstract, stylized depiction oflocal searc h.Thesolutions crowd theunshaded area, andcost decreases when wemove downw ard. Starting from aninitial solution, thealgorithm moves downhill until alocal optimum isreached. Ingeneral, thesearc hspace might beriddled with local optima, and some ofthem may beofvery poor quality .The hope isthat with ajudicious choice ofneighborhood structure , most local optima willbereasonable .Whether this isthereality ormerely misplaced faith, itisanempirical factthat local searc halgorithms arethetopperformers onabroad range of optimization problems .Let'slook atanother suchexample . 9.3.2 Graph partitioning The problem ofgraph partitioning arises inadiversity ofapplications ,from circuit layout toprogram analysis toimage segmentation. Wesawaspecial case ofit,BALANCEDCUT,in Chapter 8. GRAPHPARTITIONING Input: Anundirected graphG=(V;E)with nonnegative edge weights; areal number 2(0;1=2]. Output: Apartition ofthevertices into twogroupsAandB,eachofsize atleast jVj. Goal: Minimize thecapacity ofthecut(A;B).
9âŒ˜19âŒ˜302 Algorithms Figure 9.9Aninstance ofGRAPHPARTITIONING,with theoptimal partition for =1=2. Vertices ononeside ofthecutareshaded. Figure 9.9shows anexample inwhic hthegraph has16nodes ,alledge weights are0 or1,and theoptimal solution hascost0.Removing therestriction onthesizes ofAandB would give the MINIMUMCUTproblem, whic hweknow tobeefciently solvable using ow techniques .Thepresent variant, however ,isNP-hard. Indesigning alocal searc halgorithm, itwillbeabigconvenience tofocus onthespecial case =1=2,inwhic hAandBareforced to contain exactly halfthevertices .Theapparent lossofgenerality ispurely cosmetic ,asGRAPH PARTITIONINGreduces tothisparticular case. Weneed todecide upon aneighborhood structure forourproblem, andthere isoneobvious waytodothis.Let(A;B),withjAj=jBj,beacandidate solution; wewilldene itsneighbors tobeallsolutions obtainable byswapping onepair ofvertices across thecut, that is,all solutions oftheform(Afag+fbg;Bfbg+fag)wherea2Aandb2B.Here' sanexample ofalocal move: Wenow haveareasonable local searc hprocedure ,andwecould juststop here.Butthere isstill alotofroom forimprovement interms ofthequality ofthesolutions produced. The searc hspace includes some local optima that arequite farfrom theglobal solution. Here' s onewhic hhascost2.
9âŒ˜20âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 303 What canbedone about suchsuboptimal solutions? Wecould expand theneighborhood size toallow twoswapsatatime,butthis particular badinstance would still stubbornly resist. Instead, let'slook atsome other generic schemes forimproving local searc hprocedures . 9.3.3 Dealing with local optima Randomization and restarts Randomization canbeaninvaluable allyinlocal searc h.Itistypically used intwoways:to pickarandom initial solution, forinstance arandom graph partition; and tochoose alocal move when several areavailable . When there aremany local optima, randomization isawayofmaking sure that there isat least some probability ofgetting totheright one.Thelocal searc hcanthen berepeated several times ,with adifferent random seed oneachinvocation, andthebest solution returned. Ifthe probability ofreaching agood local optimum onanygiven runisp,then withinO(1=p)runs suchasolution islikely tobefound (recall Exercise 1.34). Figure 9.10 shows asmall instance ofgraph partitioning ,along with thesearc hspace of solutions .There areatotal of8 4 =70possible states ,butsince eachofthem hasanidentical twin inwhic htheleftandright sides ofthecutareipped, ineffect there arejust35solutions . Inthegure ,these areorganized intoseven groups forreadability .There arevelocal optima, ofwhic hfour arebad, with cost2,andoneisgood, with cost0.Iflocal searc hisstarted ata random solution, andateachstep arandom neighbor oflower costisselected, then thesearc h isatmost four times aslikely towind upinabadsolution than agood one.Thus only asmall handful ofrepetitions isneeded.
9âŒ˜21âŒ˜304 Algorithms Figure 9.10 Thesearc hspace foragraph with eight nodes .Thespace contains 35solutions , whic hhavebeen partitioned intoseven groups forclarity .Anexample ofeachisshown. There arevelocal optima. 4 states, cost 2 1 state, cost 08 states, cost 38 states, cost 44 states, cost 6 2 states, cost 4 8 states, cost 3
9âŒ˜22âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 305 Simulated annealing Intheexample ofFigure 9.10, eachrunoflocal searc hhasareasonable chance ofnding the global optimum. This isn't alwaystrue.Astheproblem size grows ,theratio ofbadtogood local optima often increases ,sometimes tothepoint ofbeing exponentially large .Insuch cases ,simply repeating thelocal searc hafewtimes isineffective . Adifferent avenue ofattac kistooccasionally allow moves that actually increase thecost, inthehope that they willpull thesearc houtofdead ends .This would bevery useful atthe badlocal optima ofFigure 9.10, forinstance .The method ofsimulated annealing redenes thelocal searc hbyintroducing thenotion ofatemperature T. letsbeanystarting solution repeat randomly chooseasolution s0intheneighborhood ofs if=cost(s0)cost(s)isnegative: replacesbys0 else: replacesbys0withprobability e=T. IfTiszero,this isidentical toourprevious local searc h.ButifTislarge ,then moves that increase thecostareoccasionally accepted. What value ofTshould beused? The trickistostart withTlarge and then gradually reduce ittozero.Thus initially , thelocal searc hcanwander around quite freely ,with only amild preference forlow-cost solutions .Astime goes on,thispreference becomes stronger ,andthesystem mostly sticksto thelower -cost region ofthesearc hspace ,with occasional excursions outofittoescape local optima. Eventually ,when thetemperature drops further ,thesystem converges onasolution. Figure 9.11 shows thisprocess schematically . Simulated annealing isinspired bythephysics ofcrystallization. When asubstance isto becrystallized, itstarts inliquid state ,with itspartic lesinrelatively unconstrained motion. Then itisslowly cooled, andasthishappens ,thepartic lesgradually move into more regular congurations .This regularity becomes more and more pronounced until nally acrystal lattice isformed. The benets ofsimulated annealing come atasignicant cost: because ofthechanging temperature andtheinitial freedom ofmovement, many more local moves areneeded until convergence .Moreover ,itisquite anarttochoose agood timetable bywhic htodecrease the temperature ,called anannealing schedule .Butinmany cases where thequality ofsolutions improves signicantly ,thetradeoff isworthwhile .
9âŒ˜23âŒ˜306 Algorithms Figure 9.11 Simulated annealing . Exercises 9.1. Inthebacktrac king algorithm for SAT,suppose that wealwayschoose asubproblem (CNF formula) that hasaclause that isassmall aspossible; andweexpand italong avariable that appears inthis small clause .Show that this isapolynomial-time algorithm inthespecial case inwhic htheinput formula hasonly clauses with twoliterals (that is,itisaninstance of2SAT). 9.2. Devise abacktrac king algorithm fortheRUDRATAPATHproblem from axed vertexs.Tofully specify suchanalgorithm youmust dene: (a)What isasubproblem? (b)How tochoose asubproblem. (c)How toexpand asubproblem. Argue briey why your choices arereasonable . 9.3. Devise abranc h-and-bound algorithm forthe SETCOVERproblem. This entails deciding: (a)What isasubproblem? (b)How doyouchoose asubproblem toexpand? (c)How doyouexpand asubproblem? (d)What isanappropriate lowerbound ? Doyouthink that your choices above willwork well ontypical instances oftheproblem? Why? 9.4. Given anundirected graphG=(V;E)inwhic heachnode hasdegreed,show how toefciently ndanindependent setwhose sizeisatleast1=(d+1)times that ofthelargest independent set. 9.5. Local searc hforminimum spanning trees.Consider thesetofallspanning trees (not justmini- mum ones) ofaweighted, connected, undirected graphG=(V;E).
9âŒ˜24âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 307 Recall from Section 5.1that adding anedgeetoaspanning treeTcreates anunique cycle,and subsequently removing anyother edgee06=efrom thiscyclegives backadifferent spanning tree T0.WewillsaythatTandT0differ byasingle edge swap (e;e0)andthat they areneighbor s. (a)Show that itispossible tomove from anyspanning treeTtoanyother spanning treeT0by performing aseries ofedge-sw aps,that is,bymoving from neighbor toneighbor .Atmost how many edge-sw apsareneeded? (b)Show that ifT0isanMST ,then itispossible tochoose these swapssothat thecosts of thespanning trees encountered along thewayarenonincreasing .Inother words ,ifthe sequence ofspanning trees encountered is T=T0!T1!T2!!Tk=T0; then cost(Ti+1)cost(Ti)foralli<k. (c)Consider thefollowing local searc halgorithm whic hisgiven asinput anundirected graph G. LetTbeanyspanning treeofG whilethereisanedge-swap (e;e0)whichreduces cost(T): T T+ee0 returnT Show that this procedure alwaysreturns aminimum spanning tree.Atmost how many iterations does ittake? 9.6. Inthe MINIMUMSTEINERTREEproblem, theinput consists of:acomplete graphG=(V;E) with distancesduvbetween allpairs ofnodes; andadistinguished setofterminal nodesV0V. Thegoal istond aminimum-cost tree that includes theverticesV0.This tree mayormaynot include nodes inVV0. Suppose thedistances intheinput areametric (recall thedenition onpage 292). Show that anefcient ratio-2 approximation algorithm for MINIMUMSTEINERTREEcanbeobtained by ignoring thenonterminal nodes andsimply returning theminimum spanning tree onV0.(Hint: Recall ourapproximation algorithm forthe TSP.) 9.7. Inthe MULTIWAYCUTproblem, theinput isanundirected graphG=(V;E)andasetofterminal nodess1;s2;:::;sk2V.Thegoal istondtheminimum setofedges inEwhose removal leaves allterminals indifferent components . (a)Show that thisproblem canbesolved exactly inpolynomial time whenk=2. (b)Give anapproximation algorithm with ratio atmost 2forthecasek=3. (c)Design alocal searc halgorithm formultiw aycut.
9âŒ˜25âŒ˜308 Algorithms 9.8. Inthe MAXSATproblem, wearegiven asetofclauses ,andwewanttond anassignment that satises asmany ofthem aspossible . (a)Show that ifthisproblem canbesolved inpolynomial time,then socan SAT. (b)Here' savery naive algorithm. foreachvariable: setitsvaluetoeither 0or1byflipping acoin Suppose theinput hasmclauses ,ofwhic hthejthhaskjliterals .Show that theexpected number ofclauses satised bythissimple algorithm is mX j=1 11 2kj m 2: Inother words ,this isa2-approximation inexpectation! And iftheclauses allcontaink literals ,then thisapproximation factor improves to1+1=(2k1). (c)Can youmake this algorithm deterministic? (Hint: Instead ofipping acoin foreach variable ,select thevalue that satises themost oftheasofyetunsatised clauses .What fraction oftheclauses issatised intheend?) 9.9. Inthe MAXIMUMCUTproblem wearegiven anundirected graphG=(V;E)with aweightw(e) oneachedge ,andwewish toseparate thevertices into twosetsSandVSsothat thetotal weight oftheedges between thetwosets isaslarge aspossible . ForeachSVdenew(S)tobethesum ofallw(e)over alledgesfu;vgsuchthatjS\fu;vgj=1. Obviously ,MAXCUTisabout maximizing w(S)over allsubsets ofV. Consider thefollowing local searc halgorithm forMAXCUT: startwithanySV whilethereisasubsetS0Vsuchthat jjS0jjSjj=1andw(S0)>w(S)do: setS=S0 (a)Show that thisisanapproximation algorithm forMAXCUTwith ratio2. (b)Butisitapolynomial-time algorithm? 9.10. Letuscallalocal searc halgorithm exact when italwaysproduces theoptimum solution. For example ,thelocal searc halgorithm fortheminimum spanning treeproblem introduced inProb- lem9.5isexact. Foranother example ,simplex canbeconsidered anexact local searc halgorithm forlinear programming . (a)Show that the2-change local searc halgorithm forthe TSPisnotexact. (b)Repeat forthedn 2e-change local searc halgorithm, wherenisthenumber ofcities . (c)Show that the(n1)-change local searc halgorithm isexact. (d)IfAisanoptimization problem, deneA-IMPROVEMENTtobethefollowing searc hproblem: Given aninstancexofAandasolutionsofA,ndanother solution ofxwith better cost(or report that none exists ,andthussisoptimum). Forexample ,inTSPIMPROVEMENTweare given adistance matrix andatour,andweareasked tondabetter tour.Itturns outthat TSPIMPROVEMENTisNP-complete ,andsoisSETCOVERIMPROVEMENT.(Can youprove this?)
9âŒ˜26âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 309 (e)Wesaythat alocal searc halgorithm haspolynomial iteration ifeachexecution oftheloop requires polynomial time.Forexample ,theobvious implementations ofthe(n1)-change local searc halgorithm forthe TSPdened above donothavepolynomial iteration. Show that, unless P=NP,there isnoexact local searc halgorithm with polynomial iteration for the TSPand SETCOVERproblems .
