ChapterâŒ˜PageâŒ˜Text
6âŒ˜0âŒ˜Chapter 6 Dynamic programming Inthepreceding chapters wehaveseen some elegant design principlesÂ—suc hasdivide-and- conquer ,graph exploration, andgreedy choiceÂ—that yield denitive algorithms foravariety ofimportant computational tasks .Thedrawbac kofthese tools isthat they canonly beused onvery specic types ofproblems .Wenow turn tothetwosledgehammer softhealgorithms craft, dynamic programming andlinear programming ,techniques ofvery broad applicability that canbeinvoked when more specialized methods fail. Predictably ,this generality often comes with acostinefciency . 6.1 Shortest paths indags, revisited Attheconclusion ofourstudy ofshortest paths (Chapter 4),weobserved that theproblem is especially easy indirected acyclicgraphs (dags). Let'srecapitulate thiscase,because itliesat theheart ofdynamic programming . Thespecial distinguishing feature ofadagisthat itsnodes canbelinearized ;that is,they canbearranged onaline sothat alledges gofrom lefttoright (Figure 6.1). Toseewhy this helps with shortest paths ,suppose wewanttogure outdistances from nodeStothe other nodes .Forconcreteness ,let'sfocus onnodeD.The only waytogettoitisthrough its Figure 6.1Adaganditslinearization (topological ordering). B D CA S E1 24 16 312 S C A B D E4 63 1 21 12 169
6âŒ˜1âŒ˜170 Algorithms predecessors ,BorC;sotondtheshortest path toD,weneed only compare these tworoutes: dist(D)=minfdist(B)+1;dist(C)+3g: Asimilar relation canbewritten forevery node .Ifwecompute thesedist values inthe left-to-right order ofFigure 6.1,wecanalwaysbesure that bythetime wegettoanodev, wealready havealltheinformation weneed tocomputedist(v).Wearetherefore able to compute alldistances inasingle pass: initialize alldist()valuesto1 dist(s)=0 foreachv2Vnfsg,inlinearized order: dist(v)=min(u;v)2Efdist(u)+l(u;v)g Notice that this algorithm issolving acollection ofsubproblems ,fdist(u):u2Vg.We start with thesmallest ofthem,dist(s),since weimmediately know itsanswer tobe0.We then proceed with progressively Â“largerÂ” subproblemsÂ—distances tovertices that arefurther andfurther along inthelinearizationÂ—where wearethinking ofasubproblem aslarge ifwe need tohavesolved alotofother subproblems before wecangettoit. This isavery general technique .Ateachnode ,wecompute some function ofthevalues ofthenode' spredecessors .Itsohappens that ourparticular function isaminimum ofsums , butwecould justaswell make itamaximum ,inwhic hcase wewould getlongest paths inthe dag.Orwecould useaproduct instead ofasum inside thebrackets,inwhic hcase wewould endupcomputing thepath with thesmallest product ofedge lengths . Dynamic programming isavery powerful algorithmic paradigm inwhic haproblem is solved byidentifying acollection ofsubproblems andtackling them onebyone,smallest rst, using theanswers tosmall problems tohelp gure outlarger ones,until thewhole lotofthem issolved. Indynamic programming wearenotgiven adag; thedagisimplicit .Itsnodes are thesubproblems wedene ,anditsedges arethedependencies between thesubproblems: if tosolve subproblem Bweneed theanswer tosubproblem A,then there isa(conceptual) edge fromAtoB.Inthiscase,Aisthought ofasasmaller subproblem thanBÂ—and itwillalways besmaller ,inanobvious sense . Butit'stime wesawanexample . 6.2 Longest increasing subsequences Inthelongest increasing subsequence problem, theinput isasequence ofnumbersa1;:::;an. Asubsequence isanysubset ofthese numbers taken inorder ,oftheformai1;ai2;:::;aikwhere 1i1<i2<<ikn,andanincreasing subsequence isoneinwhic hthenumbers are getting strictly larger .The task istond theincreasing subsequence ofgreatest length. For instance ,thelongest increasing subsequence of5;2;8;6;3;6;9;7is2;3;6;9: 5 2 8 6 3 6 9 7
6âŒ˜2âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 171 Figure 6.2Thedagofincreasing subsequences . 5 2 8 3 9 7 6 6 Inthis example ,thearrows denote transitions between consecutive elements oftheopti- malsolution. More generally ,tobetter understand thesolution space ,let'screate agraph of allpermissible transitions: establish anodeiforeachelementai,andadddirected edges (i;j) whenever itispossible foraiandajtobeconsecutive elements inanincreasing subsequence , that is,wheneveri<jandai<aj(Figure 6.2). Notice that (1)this graphG=(V;E)isadag,since alledges (i;j)havei<j,and (2) there isaone-to-one correspondence between increasing subsequences andpaths inthisdag. Therefore ,ourgoal issimply tondthelongest path inthedag! Here isthealgorithm: forj=1;2;:::;n: L(j)=1+maxfL(i):(i;j)2Eg return max jL(j) L(j)isthelength ofthelongest pathÂ—the longest increasing subsequenceÂ—ending atj(plus 1,since strictly speaking weneed tocount nodes onthepath, notedges). Byreasoning inthe same wayaswedidforshortest paths ,weseethat anypath tonodejmust pass through one ofitspredecessors ,andthereforeL(j)is1plus themaximum L()value ofthese predecessors . Ifthere arenoedges intoj,wetake themaximum over theempty set,zero.And thenal answer isthelargestL(j),since anyending position isallowed. This isdynamic programming .Inorder tosolve ouroriginal problem, wehavedened a collection ofsubproblemsfL(j):1jngwith thefollowing keyproperty that allows them tobesolved inasingle pass: (*)There isanordering onthesubproblems ,andarelation that shows how tosolve asubproblem given theanswers toÂ“smallerÂ” subproblems ,that is,subproblems that appear earlier intheordering . Inourcase,eachsubproblem issolved using therelation L(j)=1+maxfL(i):(i;j)2Eg;
6âŒ˜3âŒ˜172 Algorithms anexpression whic hinvolves only smaller subproblems .How long does this step take? It requires thepredecessors ofjtobeknown; forthistheadjacency listofthereverse graphGR, constructible inlinear time (recall Exercise 3.5), ishandy .Thecomputation ofL(j)then takes time proportional totheindegree ofj,giving anoverall running time linear injEj.This isat mostO(n2),themaximum being when theinput arrayissorted inincreasing order .Thus the dynamic programming solution isboth simple andefcient. There isonelastissue tobecleared up:theL-values only tellusthelength oftheoptimal subsequence ,sohow dowerecover thesubsequence itself? This iseasily managed with the same bookkeeping device weused forshortest paths inChapter 4.While computing L(j),we should also note downprev(j),thenext-to-last node onthelongest path toj.The optimal subsequence canthen bereconstructed byfollowing these backpointers .
6âŒ˜4âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 173 Recursion? No,thanks. Returning toourdiscussion oflongest increasing subsequences: theformula forL(j)also suggests analternative ,recursive algorithm. Wouldn't that beeven simpler? Actually ,recursion isavery badidea: theresulting procedure would require exponential time! Toseewhy,suppose that thedagcontains edges (i;j)foralli<jÂ—that is,thegiven sequence ofnumbersa1;a2;:::;anissorted. Inthat case,theformula forsubproblem L(j) becomes L(j)=1+maxfL(1);L(2);:::;L(j1)g: Thefollowing gure unra vels therecursion forL(5).Notice that thesame subproblems get solved over andover again! L(2) L(1) L(1) L(2) L(1) L(2) L(1) L(1) L(1) L(2) L(1)L(3)L(1) L(3) L(4)L(5) ForL(n)this tree hasexponentially many nodes (can youbound it?), and soarecursive solution isdisastrous . Then why didrecursion work sowell with divide-and-conquer? The keypoint isthat in divide-and-conquer ,aproblem isexpressed interms ofsubproblems that aresubstantiall y smaller ,sayhalf thesize.Forinstance ,mergesort sorts anarrayofsizenbyrecursively sorting two subarra ysofsizen=2.Because ofthis sharp drop inproblem size,thefull recursion tree hasonly logarithmic depth andapolynomial number ofnodes . Incontrast, inatypical dynamic programming formulation, aproblem isreduced to subproblems that areonly slightly smaller Â—for instance ,L(j)relies onL(j1).Thus the fullrecursion tree generally haspolynomial depth and anexponential number ofnodes . However ,itturns outthat most ofthese nodes arerepeats ,that there arenottoomany distinct subproblems among them. Efciency istherefore obtained byexplicitly enumerating thedistinct subproblems andsolving them intheright order .
6âŒ˜5âŒ˜174 Algorithms Programming? The origin oftheterm dynamic programming hasvery little todowith writing code.It wasrst coined byRichard Bellman inthe1950s ,atime when computer programming was anesoteric activity practiced bysofewpeople astonoteven merit aname .Backthen programming meant Â“planning ,Â”and Â“dynamic programmingÂ” wasconceived tooptimally plan multistage processes .ThedagofFigure 6.2canbethought ofasdescribing thepossible waysinwhic hsuchaprocess canevolve: eachnode denotes astate ,theleftmost node isthe starting point, andtheedges leaving astate represent possible actions ,leading todifferent states inthenext unit oftime. Theetymology oflinear programming ,thesubject ofChapter 7,issimilar . 6.3 Edit distance When aspell checkerencounters apossible misspelling ,itlooks initsdictionary forother words that arecloseby.What istheappropriate notion ofcloseness inthiscase? Anatural measure ofthedistance between twostrings istheextent towhic hthey canbe aligned ,ormatc hedup.Technically ,analignment issimply awayofwriting thestrings one above theother .Forinstance ,here aretwopossible alignments ofSNOWY andSUNNY : SNOWY SUNNY Cost: 3SNOWY SUNNY Cost: 5 The Â“Â”indicates aÂ“gapÂ”; anynumber ofthese canbeplaced ineither string .Thecostofan alignment isthenumber ofcolumns inwhic htheletters differ .And theeditdistance between twostrings isthecost oftheir best possible alignment. Doyouseethat there isnobetter alignment ofSNOWY andSUNNY than theoneshown here with acostof3? Edit distance issonamed because itcanalso bethought ofastheminimum number of edits Â—insertions ,deletions ,and substitutions ofcharactersÂ—needed totransform therst string into thesecond. Forinstance ,thealignment shown ontheleftcorresponds tothree edits: insertU,substitute O!N,anddeleteW. Ingeneral, there aresomany possible alignments between twostrings that itwould be terribly inefcient tosearc hthrough allofthem forthebest one.Instead weturn todynamic programming . Adynamic programming solution When solving aproblem bydynamic programming ,themost crucial question is,Whatarethe subproblems? Aslong asthey arechosen soastohavetheproperty (*)from page 171. itisan easy matter towrite down thealgorithm: iteratively solve onesubproblem after theother ,in order ofincreasing size. Our goal istondtheeditdistance between twostringsx[1m]andy[1n].What isa good subproblem? Well,itshould gopart ofthewaytowardsolving thewhole problem; sohow
6âŒ˜6âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 175 Figure 6.3Thesubproblem E(7;5). P LYNOM AL I OPON N LA XE E TI about looking attheedit distance between some prex oftherst string ,x[1i],andsome prex ofthesecond,y[1j]?Call thissubproblem E(i;j)(seeFigure 6.3). Ournal objective , then, istocomputeE(m;n). Forthis towork, weneed tosomehow expressE(i;j)interms ofsmaller subproblems . Let'sseeÂ—what doweknow about thebest alignment betweenx[1i]andy[1j]?Well,its rightmost column canonly beoneofthree things: x[i] or y[j]orx[i] y[j] Therst case incurs acostof1forthisparticular column, anditremains toalignx[1i1] withy[1j].Butthisisexactly thesubproblem E(i1;j)!Weseem tobegetting somewhere . Inthesecond case,also with cost1,westill need toalignx[1i]withy[1j1].This is again another subproblem, E(i;j1).And inthenal case,whic heither costs 1(ifx[i]6=y[j]) or0(ifx[i]=y[j]),what' sleftisthesubproblem E(i1;j1).Inshort, wehaveexpressed E(i;j)interms ofthree smaller subproblems E(i1;j),E(i;j1),E(i1;j1).Wehaveno idea whic hofthem istheright one,soweneed totrythem allandpickthebest: E(i;j)=minf1+E(i1;j);1+E(i;j1);diff(i;j)+E(i1;j1)g where forconvenience diff(i;j)isdened tobe0ifx[i]=y[j]and1otherwise . Forinstance ,incomputing theedit distance betweenEXPONENTIAL andPOLYNOMIAL , subproblem E(4;3)corresponds totheprexesEXPO andPOL.Therightmost column oftheir best alignment must beoneofthefollowing: O or LorO L Thus ,E(4;3)=minf1+E(3;3);1+E(4;2);1+E(3;2)g. Theanswers toallthesubproblems E(i;j)form atwo-dimensional table ,asinFigure 6.4. Inwhat order should these subproblems besolved? Any order isne,aslong asE(i1;j), E(i;j1),andE(i1;j1)arehandled beforeE(i;j).Forinstance ,wecould llinthetable onerowatatime,from toprowtobottom row,andmoving lefttoright across eachrow.Or alternatively ,wecould llitincolumn bycolumn. Both methods would ensure that bythe time wegetaround tocomputing aparticular table entry ,alltheother entries weneed are already lled in.
6âŒ˜7âŒ˜176 Algorithms Figure 6.4(a)Thetable ofsubproblems .EntriesE(i1;j1),E(i1;j),andE(i;j1)are needed tollinE(i;j).(b)Thenal table ofvalues found bydynamic programming . (a) ij1j i1 mGOALn(b) POLYNOM IAL 012345678910 E112345678910 X222345678910 P323345678910 O43234556789 N54334456789 E65444556789 N76555456789 T87666556789 I98777666678 A109888777767 L1110989888876 Withboth thesubproblems and theordering specied, wearealmost done .There just remain theÂ“base casesÂ” ofthedynamic programming ,thevery smallest subproblems .Inthe present situation, these areE(0;)andE(;0),both ofwhic hareeasily solved.E(0;j)isthe editdistance between the0-length prex ofx,namely theempty string ,andtherstjletters ofy:clearly ,j.And similarly ,E(i;0)=i. Atthispoint, thealgorithm foreditdistance basically writes itself . fori=0;1;2;:::;m: E(i;0)=i forj=1;2;:::;n: E(0;j)=j fori=1;2;:::;m: forj=1;2;:::;n: E(i;j)=minfE(i1;j)+1;E(i;j1)+1;E(i1;j1)+diff(i;j)g returnE(m;n) This procedure llsinthetable rowbyrow,andlefttoright within eachrow.Eachentry takes constant time tollin,sotheoverall running time isjustthesizeofthetable ,O(mn). And inourexample ,theeditdistance turns outtobe6: EXPONENTIAL POLYNOMIAL
6âŒ˜8âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 177 Figure 6.5Theunderlying dag,andapath oflength 6. POLYNOM AL I E X P O N E N T A LI The underlying dag Every dynamic program hasanunderlying dagstructure: think ofeachnode asrepresenting a subproblem, andeachedge asaprecedence constraint ontheorder inwhic hthesubproblems canbetackled. Having nodesu1;:::;ukpoint tovmeans Â“subproblem vcanonly besolved once theanswers tou1;:::;ukareknown. Â” Inourpresent edit distance application, thenodes oftheunderlying dagcorrespond to subproblems ,orequivalently ,topositions (i;j)inthetable .Itsedges aretheprecedence constraints ,oftheform(i1;j)!(i;j),(i;j1)!(i;j),and(i1;j1)!(i;j)(Figure 6.5). Infact, wecantake things alittle further and putweights ontheedges sothat theedit distances aregiven byshortest paths inthedag! Toseethis,setalledge lengths to1,except forf(i1;j1)!(i;j):x[i]=y[j]g(shown dotted inthegure), whose length is0.The nal answer isthen simply thedistance between nodess=(0;0)andt=(m;n).One possible shortest path isshown, theonethat yields thealignment wefound earlier .Onthispath, each move down isadeletion, eachmove right isaninsertion, andeachdiagonal move iseither a matc horasubstitution. Byaltering theweights onthis dag,wecanallow generalized forms ofedit distance ,in whic hinsertions ,deletions ,andsubstitutions havedifferent associated costs .
6âŒ˜9âŒ˜178 Algorithms Common subproblems Finding theright subproblem takes creativity and experimentation. But there areafew standard choices that seem toarise repeatedly indynamic programming . i.Theinput isx1;x2;:::;xnandasubproblem isx1;x2;:::;xi. x1x2x3x4x5x6x7x8x9x10 Thenumber ofsubproblems istherefore linear . ii.Theinput isx1;:::;xn,andy1;:::;ym.Asubproblem isx1;:::;xiandy1;:::;yj. x1x2x3x4x5x6x7x8x9x10 y1y2y3y4y5y6y7y8 Thenumber ofsubproblems isO(mn). iii.Theinput isx1;:::;xnandasubproblem isxi;xi+1;:::;xj. x1x2x3x4x5x6x7x8x9x10 Thenumber ofsubproblems isO(n2). iv.Theinput isarooted tree.Asubproblem isarooted subtree . Ifthetree hasnnodes ,how many subproblems arethere? We'vealready encountered therst twocases ,andtheothers arecoming upshortly .
6âŒ˜10âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 179
6âŒ˜11âŒ˜180 Algorithms Ofmice and men Our bodies areextraordinary machines: exible infunction, adaptive tonew environments , andable tointeract andreproduce .Allthese capabilities arespecied byaprogram unique toeachofus,astring that is3billion characters long over thealphabetfA;C;G;TgÂ—our DNA. TheDNA sequences ofanytwopeople differ byonly about 0:1%.However ,thisstillleaves 3million positions onwhic hthey vary,more than enough toexplain thevast range ofhuman diversity .These differences areofgreat scientic andmedical interestÂ—for instance ,they might help predict whic hpeople areprone tocertain diseases . DNA isavast andseemingly inscrutable program, butitcanbebroken down intosmaller units that aremore specic intheir role,rather like subroutines .These arecalled genes . Computers havebecome acrucial tool inunderstanding thegenes ofhumans and other organisms ,totheextent that computational genomics isnow aeld initsown right. Here areexamples oftypical questions that arise . 1.When anew gene isdiscovered, onewaytogain insight into itsfunction istond known genes that matc hitclosely .This isparticularly helpful intransferring knowl- edge from well-studied species ,suchasmice ,tohuman beings . Abasic primitive inthissearc hproblem istodene anefciently computable notion of when twostrings approximately matc h.The biology suggests ageneralization ofedit distance ,anddynamic programming canbeused tocompute it. Then there' stheproblem ofsearc hing through thevast thicketofknown genes: the database GenBank already hasatotal length ofover1010,andthisnumber isgrowing rapidly .The current method ofchoice isBLAST ,aclever combination ofalgorithmic tricksandbiological intuitions that hasmade itthemost widely used softw areincom- putational biology . 2.Methods forsequencing DNA (that is,determining thestring ofcharacters that consti- tute it)typically only ndfragments of500Â–700 characters .Billions ofthese randomly scattered fragments canbegenerated, buthow canthey beassembled into acoherent DNA sequence? Foronething ,theposition ofanyonefragment inthenal sequence isunknown andmust beinferred bypiecing together overlapping fragments . Ashowpiece ofthese efforts isthedraft ofhuman DNA completed in2001 bytwo groups simultaneously: thepublic lyfunded Human Genome Consortium andthepri- vate Celera Genomics . 3.When aparticular gene hasbeen sequenced ineachofseveral species ,canthis infor - mation beused toreconstruct theevolutionary history ofthese species? Wewill explore these problems intheexercises attheendofthis chapter .Dynamic pro- gramming hasturned outtobeaninvaluable toolforsome ofthem andforcomputational biology ingeneral.
6âŒ˜12âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 181 6.4 Knapsack During arobbery ,aburglar nds muchmore lootthan hehadexpected andhastodecide what totake.Hisbag(orÂ“knapsac kÂ”)willhold atotal weight ofatmostWpounds .There aren items topickfrom, ofweightw1;:::;wnanddollar valuev1;:::;vn.What' sthemost valuable combination ofitems hecantinto hisbag?1 Forinstance ,takeW=10and Item Weight Value 1 6 $30 2 3 $14 3 4 $16 4 2 $9 There aretwoversions ofthis problem. Ifthere areunlimited quantities ofeachitem avail- able,theoptimal choice istopickitem 1andtwoofitem 4(total: $48). Ontheother hand, ifthere isoneofeachitem (the burglar hasbroken into anartgallery ,say),then theoptimal knapsac kcontains items 1and3(total: $46). Asweshall seeinChapter 8,neither version ofthisproblem islikely tohaveapolynomial- time algorithm. However ,using dynamic programming they canboth besolved inO(nW) time,whic hisreasonable whenWissmall, butisnotpolynomial since theinput size is proportional tologWrather thanW. Knapsack with repetition Let'sstart with theversion that allows repetition. Asalways,themain question indynamic programming is,what arethesubproblems? Inthiscase wecanshrink theoriginal problem intwoways:wecaneither look atsmaller knapsac kcapacities wW,orwecanlook at fewer items (forinstance ,items 1;2;:::;j,forjn).Itusually takes alittle experimentation togure outexactly what works . Therst restriction calls forsmaller capacities .Accordingly ,dene K(w)=maximum value achievable with aknapsac kofcapacityw: Can weexpress this interms ofsmaller subproblems? Well,iftheoptimal solution toK(w) includes itemi,then removing this item from theknapsac kleavesanoptimal solution to K(wwi).Inother words ,K(w)issimplyK(wwi)+vi,forsomei.Wedon't know whic hi, soweneed totryallpossibilities . K(w)=max i:wiwfK(wwi)+vig; where asusual ourconvention isthat themaximum over anempty setis0.We'redone! The algorithm now writes itself ,anditischaracteristically simple andelegant. 1Ifthisapplication seems frivolous ,replace Â“weightÂ” with Â“CPU timeÂ” andÂ“only Wpounds canbetakenÂ” with Â“only Wunits ofCPU time areavailable .Â”OruseÂ“bandwidthÂ” inplace ofÂ“CPU time,Â”etc.Theknapsac kproblem generalizes awide variety ofresource-constrained selection tasks .
6âŒ˜13âŒ˜182 Algorithms K(0)=0 forw=1toW: K(w)=maxfK(wwi)+vi:wiwg returnK(W) This algorithm llsinaone-dimensional table oflengthW+1,inleft-to-right order .Each entry cantake uptoO(n)time tocompute ,sotheoverall running time isO(nW). Asalways,there isanunderlying dag.Tryconstructing it,andyouwillberewarded with astartling insight: thisparticular variant ofknapsac kboils down tonding thelongest path inadag! Knapsack without repetition Ontothesecond variant: what ifrepetitions arenotallowed? Our earlier subproblems now become completely useless .Forinstance ,knowing that thevalueK(wwn)isvery high doesn't help us,because wedon't know whether ornotitemnalready gotused upinthis partial solution. Wemust therefore rene ourconcept ofasubproblem tocarry additional information about theitems being used. Weaddasecond parameter ,0jn: K(w;j)=maximum value achievable using aknapsac kofcapacitywanditems 1;:::;j: Theanswer weseek isK(W;n). How canweexpress asubproblem K(w;j)interms ofsmaller subproblems? Quite simple: either itemjisneeded toachieve theoptimal value ,oritisn't needed: K(w;j)=maxfK(wwj;j1)+vj;K(w;j1)g: (The rst case isinvoked only ifwjw.)Inother words ,wecanexpressK(w;j)interms of subproblems K(;j1). The algorithm then consists oflling outatwo-dimensional table ,withW+1rows and n+1columns .Eachtable entry takes just constant time,soeven though thetable ismuch larger than intheprevious case,therunning time remains thesame ,O(nW).Here' sthecode. Initialize allK(0;j)=0andallK(w;0)=0 forj=1ton: forw=1toW: ifwj>w:K(w;j)=K(w;j1) else:K(w;j)=maxfK(w;j1);K(wwj;j1)+vjg returnK(W;n)
6âŒ˜14âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 183 Memoization Indynamic programming ,wewrite outarecursive formula that expresses large problems interms ofsmaller ones andthen useittolloutatable ofsolution values inabottom-up manner ,from smallest subproblem tolargest. Theformula also suggests arecursive algorithm, butwesawearlier that naive recursion canbeterribly inefcient, because itsolves thesame subproblems over and over again. What about amore intelligent recursive implementation, onethat remembers itsprevious invocations andthereby avoids repeating them? Ontheknapsac kproblem (with repetitions), suchanalgorithm would useahash table (recall Section 1.5) tostore thevalues ofK()that had already been computed. Ateach recursive callrequesting someK(w),thealgorithm would rst checkiftheanswer was already inthetable andthen would proceed toitscalculation only ifitwasn't. This trickis called memoization : Ahashtable,initially empty,holdsvaluesofK(w)indexed byw function knapsack (w) ifwisinhashtable: returnK(w) K(w)=maxfknapsack (wwi)+vi:wiwg insertK(w)intohashtable,withkeyw returnK(w) Since this algorithm never repeats asubproblem, itsrunning time isO(nW),just likethe dynamic program. However ,theconstant factor inthisbig-Onotation issubstantially larger because oftheoverhead ofrecursion. Insome cases ,though, memoization paysoff.Here' swhy: dynamic programming au- tomatically solves every subproblem that could conceivably beneeded, while memoization only ends upsolving theones that areactually used. Forinstance ,suppose thatWandall theweightswiaremultiples of100.Then asubproblem K(w)isuseless if100does notdivide w.Thememoized recursive algorithm willnever look atthese extraneous table entries .
6âŒ˜15âŒ˜184 Algorithms Figure 6.6ABCD=(A(BC))D. (a)  C D B A 201110 5020 10100(b)  A BC 5020 2010D 10100 (c) A(BC) 5010D 10100(d) (A(BC))D 50100 6.5 Chain matrix multiplication Suppose that wewanttomultiply four matrices ,ABCD,ofdimensions 5020,201, 110,and10100,respectively (Figure 6.6). This willinvolve iteratively multiplying two matrices atatime.Matrix multiplication isnotcommutative (ingeneral,AB6=BA),butit isassociative ,whic hmeans forinstance thatA(BC)=(AB)C.Thus wecancompute ourproduct offour matrices inmany different ways,depending onhow weparenthesize it. Aresome ofthese better than others? Multiplying anmnmatrix byannpmatrix takesmnp multiplications ,toagood enough approximation. Using thisformula, let'scompare several different waysofevaluating ABCD: Parenthesization Cost computation Cost A((BC)D)20110+2010100+5020100120;200 (A(BC))D20110+502010+5010100 60;200 (AB)(CD)50201+110100+501100 7;000 Asyoucansee,theorder ofmultiplications makes abigdifference inthenal running time! Moreover ,thenatural greedy approac h,toalwaysperform thecheapest matrix multiplication available ,leads tothesecond parenthesization shown here andistherefore afailure . How dowedetermine theoptimal order ,ifwewanttocomputeA1A2An,where theAi'sarematrices with dimensions m0m1;m1m2;:::;mn1mn,respectively? The rst thing tonotice isthat aparticular parenthesization canberepresented very naturally by abinary tree inwhic htheindividual matrices correspond totheleaves,theroot isthenal
6âŒ˜16âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 185 Figure 6.7(a)((AB)C)D;(b)A((BC)D);(c)(A(BC))D. D A C B AD B CD A C B(a) (b) (c) product, and interior nodes areintermediate products (Figure 6.7). The possible orders in whic htodothemultiplication correspond tothevarious fullbinary trees withnleaves,whose number isexponential inn(Exercise 2.13). Wecertainly cannot tryeachtree,andwith brute force thus ruled out,weturn todynamic programming . The binary trees ofFigure 6.7aresuggestive: foratree tobeoptimal, itssubtrees must also beoptimal. What arethesubproblems corresponding tothesubtrees? They areproducts oftheformAiAi+1Aj.Let'sseeifthisworks: for1ijn,dene C(i;j)=minimum costofmultiplying AiAi+1Aj. The size ofthis subproblem isthenumber ofmatrix multiplications ,jjij.The smallest subproblem iswheni=j,inwhic hcase there' snothing tomultiply ,soC(i;i)=0.Forj>i, consider theoptimal subtree forC(i;j).The rst branc hinthis subtree ,theoneatthetop, willsplit theproduct intwopieces ,oftheformAiAkandAk+1Aj,forsomek betweeniandj.The cost ofthesubtree isthen thecost ofthese twopartial products ,plus thecostofcombining them:C(i;k)+C(k+1;j)+mi1mkmj.And wejustneed tondthe splitting pointkforwhic hthisissmallest: C(i;j)=min ik<jfC(i;k)+C(k+1;j)+mi1mkmjg: Weareready tocode! Inthefollowing ,thevariablesdenotes subproblem size. fori=1ton:C(i;i)=0 fors=1ton1: fori=1tons: j=i+s C(i;j)=minfC(i;k)+C(k+1;j)+mi1mkmj:ik<jg returnC(1;n) The subproblems constitute atwo-dimensional table ,eachofwhose entries takesO(n)time tocompute .Theoverall running time isthusO(n3).
6âŒ˜17âŒ˜186 Algorithms Figure 6.8Wewantapath fromstotthat isboth short andhasfewedges . B D CA S1 212 14 T5 35 6.6 Shortest paths Westarted this chapter with adynamic programming algorithm fortheelementary task of nding theshortest path inadag.Wenow turn tomore sophisticated shortest-path problems andseehow these toocanbeaccommodated byourpowerful algorithmic technique . Shortest reliable paths Life iscomplicated, andabstractions suchasgraphs ,edge lengths ,andshortest paths rarely capture thewhole truth. Inacommunications network, forexample ,even ifedge lengths faithfully reect transmission delays,there maybeother considerations involved inchoosing apath. Forinstance ,eachextra edge inthepath might beanextra Â“hopÂ” fraught with uncer - tainties anddangers ofpacketloss.Insuchcases ,wewould liketoavoid paths with toomany edges .Figure 6.8illustrates this problem with agraph inwhic htheshortest path fromSto Thasfour edges ,while there isanother path that isalittle longer butuses only twoedges .If four edges translate toprohibitive unreliability ,wemayhavetochoose thelatter path. Suppose then that wearegiven agraphGwith lengths ontheedges ,along with twonodes sandtandanintegerk,andwewanttheshortest path fromstotthat uses atmostkedges . Isthere aquickwaytoadapt Dijkstra' salgorithm tothis new task? Not quite: that algorithm focuses onthelength ofeachshortest path without Â“rememberingÂ” thenumber of hops inthepath, whic hisnow acrucial piece ofinformation. Indynamic programming ,thetrickistochoose subproblems sothat allvital information isremembered and carried forward. Inthis case,letusdene ,foreachvertexvand each integerik,dist(v;i)tobethelength oftheshortest path fromstovthat usesiedges .The starting valuesdist(v;0)are1forallvertices excepts,forwhic hitis0.And thegeneral update equation is,naturally enough, dist(v;i)=min (u;v)2Efdist(u;i1)+`(u;v)g: Need wesaymore?
6âŒ˜18âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 187 All-pairs shortest paths What ifwewanttond theshortest path notjust betweensandtbutbetween allpairs ofvertices? One approac hwould betoexecute ourgeneral shortest-path algorithm from Section 4.6.1 (since there maybenegative edges)jVjtimes ,once foreachstarting node .The total running time would then beO(jVj2jEj).We'llnow seeabetter alternative ,theO(jVj3) dynamic programming-based Floyd-W arshall algorithm . Isthere isagood subproblem forcomputing distances between allpairs ofvertices ina graph? Simply solving theproblem formore andmore pairs orstarting points isunhelpful, because itleads right backtotheO(jVj2jEj)algorithm. One idea comes tomind: theshortest pathu!w1!!wl!vbetweenuandv uses some number ofintermediate nodesÂ—possibly none .Suppose wedisallow intermediate nodes altogether .Then wecansolve all-pairs shortest paths atonce: theshortest path from utovissimply thedirect edge(u;v),ifitexists .What ifwenow gradually expand theset ofpermissible intermediate nodes ?Wecandothis onenode atatime,updating theshortest path lengths ateachstage .Eventually this setgrows toallofV,atwhic hpoint allvertices areallowed tobeonallpaths ,andwehavefound thetrue shortest paths between vertices of thegraph! More concretely ,number thevertices inVasf1;2;:::;ng,andletdist(i;j;k)denote the length oftheshortest path fromitojinwhic honly nodesf1;2;:::;kgcanbeused asinterme- diates .Initially ,dist(i;j;0)isthelength ofthedirect edge betweeniandj,ifitexists ,andis 1otherwise . What happens when weexpand theintermediate settoinclude anextra nodek?Wemust reexamine allpairsi;jandcheckwhether usingkasanintermediate point gives usashorter path fromitoj.Butthis iseasy: ashortest path fromitojthat useskalong with possibly other lower -numbered intermediate nodes goes throughkjustonce (why? because weassume that there arenonegative cycles). And wehavealready calculated thelength oftheshortest path fromitokandfromktojusing only lower -numbered vertices: dist(k;j;k1)k jdist(i;k;k1) i dist(i;j;k1) Thus ,usingkgives usashorter path fromitojifandonly if dist(i;k;k1)+dist(k;j;k1)<dist(i;j;k1); inwhic hcasedist(i;j;k)should beupdated accordingly . Here istheFloyd-W arshall algorithmÂ—and asyoucansee,ittakesO(jVj3)time. fori=1ton: forj=1ton: dist(i;j;0)=1
6âŒ˜19âŒ˜188 Algorithms Figure 6.9Theoptimal traveling salesman tour haslength 10. B D CA E 22 23 424 21 3 forall(i;j)2E: dist(i;j;0)=`(i;j) fork=1ton: fori=1ton: forj=1ton: dist(i;j;k)=minfdist(i;k;k1)+dist(k;j;k1);dist(i;j;k1)g The traveling salesman problem Atraveling salesman isgetting ready forabigsales tour.Starting athishometown, suitcase inhand, hewill conduct ajourney inwhic heachofhistarget cities isvisited exactly once before hereturns home .Given thepairwise distances between cities ,what isthebest order inwhic htovisit them, soastominimize theoverall distance traveled? Denote thecities by1;:::;n,thesalesman' shometown being 1,and letD=(dij)bethe matrix ofintercity distances .The goal istodesign atour that starts andends at1,includes allother cities exactly once,and hasminimum total length. Figure 6.9shows anexample involving vecities .Can youspot theoptimal tour? Even inthistiny example ,itistrickyfor ahuman tondthesolution; imagine what happens when hundreds ofcities areinvolved. Itturns outthis problem isalso difcult forcomputers .Infact, thetraveling salesman problem (TSP) isoneofthemost notorious computational tasks .There isalong history of attempts atsolving it,along saga offailures andpartial successes ,andalong theway,major advances inalgorithms and complexity theory .The most basic piece ofbadnews about the TSP,whic hwewillbetter understand inChapter 8,isthat itishighly unlikely tobesolvable inpolynomial time. How long does ittake,then? Well,thebrute-force approac histoevaluate every possible tour andreturn thebest one.Since there are(n1)!possibilities ,this strategy takesO(n!) time.Wewillnow seethat dynamic programming yields amuchfaster solution, though nota polynomial one. What istheappropriate subproblem fortheTSP? Subproblems refer topartial solutions , and inthis case themost obvious partial solution istheinitial portion ofatour.Suppose wehavestarted atcity1asrequired, havevisited afewcities ,andarenow incityj.What
6âŒ˜20âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 189 information doweneed inorder toextend thispartial tour? Wecertainly need toknowj,since thiswilldetermine whic hcities aremost convenient tovisit next. And wealso need toknow allthecities visited sofar,sothat wedon't repeat anyofthem. Here ,then, isanappropriate subproblem. Forasubset ofcitiesSf1;2;:::;ngthat includes 1,andj2S,letC(S;j)bethe length oftheshortest path visiting eachnode inSexactly once,starting at1and ending atj. WhenjSj>1,wedeneC(S;1)=1since thepath cannot both start andendat1. Now,let'sexpressC(S;j)interms ofsmaller subproblems .Weneed tostart at1andend atj;what should wepickasthesecond-to-last city? Ithastobesomei2S,sotheoverall path length isthedistance from1toi,namely ,C(Sfjg;i),plus thelength ofthenal edge, dij.Wemust pickthebest suchi: C(S;j)=min i2S:i6=jC(Sfjg;i)+dij: Thesubproblems areordered byjSj.Here' sthecode. C(f1g;1)=0 fors=2ton: forallsubsetsSf1;2;:::;ngofsizesandcontaining 1: C(S;1)=1 forallj2S;j6=1: C(S;j)=minfC(Sfjg;i)+dij:i2S;i6=jg return minjC(f1;:::;ng;j)+dj1 There areatmost 2nnsubproblems ,and eachonetakes linear time tosolve .The total running time isthereforeO(n22n). 6.7 Independent sets intrees Asubset ofnodesSVisanindependent setofgraphG=(V;E)ifthere arenoedges between them. Forinstance ,inFigure 6.10 thenodesf1;5gform anindependent set,but nodesf1;4;5gdonot, because oftheedge between 4and5.The largest independent setis f2;3;6g. Like several other problems wehaveseen inthischapter (knapsac k,traveling salesman), nding thelargest independent setinagraph isbelieved tobeintractable .However ,when thegraph happens tobeatree,theproblem canbesolved inlinear time,using dynamic programming .And what aretheappropriate subproblems? Already inthechain matrix multiplication problem wenoticed that thelayered structure ofatree provides anatural denition ofasubproblemÂ—as long asonenode ofthetree hasbeen identied asaroot. Sohere' sthealgorithm: Start byrooting thetree atanynoder.Now,eachnode denes a subtreeÂ—the onehanging from it.This immediately suggests subproblems: I(u)=sizeoflargest independent setofsubtree hanging fromu:
6âŒ˜21âŒ˜190 Algorithms Ontime and memory Theamount oftime ittakes torunadynamic programming algorithm iseasy todiscern from thedagofsubproblems: inmany cases itisjustthetotal number ofedges inthedag! All wearereally doing isvisiting thenodes inlinearized order ,examining eachnode' sinedges , and, most often, doing aconstant amount ofwork peredge.Bytheend, eachedge ofthedag hasbeen examined once. Buthow muchcomputer memory isrequired? There isnosimple parameter ofthedag characterizing this.Itiscertainly possible todothejobwith anamount ofmemory propor - tional tothenumber ofvertices (subproblems), butwecanusually getawaywith muchless. Thereason isthat thevalue ofaparticular subproblem only needs toberemembered until thelarger subproblems depending onithavebeen solved. Thereafter ,thememory ittakes upcanbereleased forreuse . Forexample ,intheFloyd-W arshall algorithm thevalue ofdist(i;j;k)isnotneeded once thedist(;;k+1)values havebeen computed. Therefore ,weonly need twojVjjVjarrays tostore thedist values ,oneforoddvalues ofkandoneforeven values: when computing dist(i;j;k),weoverwrite dist(i;j;k2). (And letusnotforget that, asalwaysindynamic programming ,wealsoneed onemore ar- ray,prev(i;j),storing thenext tolastvertex inthecurrent shortest path fromitoj,avalue that must beupdated withdist(i;j;k).Weomit thismundane butcrucial bookkeeping step from ourdynamic programming algorithms .) Can youseewhy theeditdistance daginFigure 6.5only needs memory proportional to thelength oftheshorter string? Our nal goal isI(r). Dynamic programming proceeds asalwaysfrom smaller subproblems tolarger ones,that istosay,bottom-up intherooted tree.Suppose weknow thelargest independent sets forall subtrees below acertain nodeu;inother words ,suppose weknowI(w)foralldescendants w ofu.How canwecomputeI(u)?Let'ssplit thecomputation into twocases: anyindependent seteither includesuoritdoesn't (Figure 6.11). I(u)=max8 < :1+X grandchildren wofuI(w);X children wofuI(w)9 = ;: Iftheindependent setincludesu,then wegetonepoint forit,butwearen't allowed toinclude thechildren ofuÂ—therefore wemove ontothegrandc hildren. This istherst case inthe formula. Ontheother hand, ifwedon't includeu,then wedon't getapoint forit,butwecan move ontoitschildren. The number ofsubproblems isexactly thenumber ofvertices .Withalittle care,the running time canbemade linear ,O(jVj+jEj).
6âŒ˜22âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 191 Figure 6.10 Thelargest independent setinthisgraph hassize3. 1 2 3 45 6 Figure 6.11I(u)isthesize ofthelargest independent setofthesubtree rooted atu.Two cases: eitheruisinthisindependent set,oritisn't. r u Exercises 6.1. Acontiguous subsequence ofalistSisasubsequence made upofconsecutive elements ofS.For instance ,ifSis 5;15;30;10;5;40;10; then15;30;10isacontiguous subsequence but5;15;40isnot. Give alinear -time algorithm for thefollowing task: Input: Alistofnumbers ,a1;a2;:::;an. Output: Thecontiguous subsequence ofmaximum sum (asubsequence oflength zero hassum zero). Forthepreceding example ,theanswer would be10;5;40;10,with asum of55. (Hint: Foreachj2f1;2;:::;ng,consider contiguous subsequences ending exactly atpositionj.) 6.2. Youaregoing onalong trip.Youstart ontheroad atmile post 0.Along thewaythere aren hotels ,atmile postsa1<a2<<an,where eachaiismeasured from thestarting point. The only places youareallowed tostop areatthese hotels ,butyoucanchoose whic hofthehotels youstop at.Youmust stop atthenal hotel (atdistancean),whic hisyour destination.
6âŒ˜23âŒ˜192 Algorithms You'dideally liketotravel200miles aday,butthismaynotbepossible (depending onthespacing ofthehotels). Ifyoutravelxmiles during aday,thepenalty forthat dayis(200x)2.Youwant toplan your tripsoastominimize thetotal penaltyÂ—that is,thesum, over alltraveldays,ofthe daily penalties . Give anefcient algorithm that determines theoptimal sequence ofhotels atwhic htostop. 6.3. Yuckdonald' sisconsidering opening aseries ofrestaurants along Quaint Valley Highw ay(QVH). Thenpossible locations arealong astraight line,andthedistances ofthese locations from the start ofQVH are,inmiles andinincreasing order ,m1;m2;:::;mn.Theconstraints areasfollows: Ateachlocation, Yuckdonald' smayopen atmost onerestaurant. Theexpected prot from opening arestaurant atlocationiispi,wherepi>0andi=1;2;:::;n. Any tworestaurants should beatleastkmiles apart, wherekisapositive integer . Give anefcient algorithm tocompute themaximum expected total prot subject tothegiven constraints . 6.4. Youaregiven astring ofncharacterss[1:::n],whic hyoubelieve tobeacorrupted textdocument inwhic hallpunctuation hasvanished (sothat itlooks something likeÂ“itwasthebestoftimes ...Â”). Youwish toreconstruct thedocument using adictionary ,whic hisavailable intheform ofa Boolean functiondict():foranystringw, dict(w)=true ifwisavalid word false otherwise . (a)Give adynamic programming algorithm that determines whether thestrings[]canbe reconstituted asasequence ofvalid words .The running time should beatmostO(n2), assuming calls todict take unit time. (b)Intheevent that thestring isvalid, make your algorithm output thecorresponding se- quence ofwords . 6.5. Pebbling acheckerboard. Wearegiven acheckerboard whic hhas4rows andncolumns ,and hasaninteger written ineachsquare .Wearealso given asetof2npebbles ,and wewantto place some orallofthese onthecheckerboard (eachpebble canbeplaced onexactly onesquare) soastomaximize thesum oftheintegers inthesquares that arecovered bypebbles .There is oneconstraint: foraplacement ofpebbles tobelegal, notwoofthem canbeonhorizontally or vertically adjacent squares (diagonal adjacency isne). (a)Determine thenumber oflegal patterns that canoccur inanycolumn (inisolation, ignoring thepebbles inadjacent columns) anddescribe these patterns . Call twopatterns compatible ifthey canbeplaced onadjacent columns toform alegal placement. Letusconsider subproblems consisting oftherstkcolumns 1kn.Eachsubproblem can beassigned atype,whic histhepattern occurring inthelastcolumn. (b)Using thenotions ofcompatibility andtype,give anO(n)-time dynamic programming algo- rithm forcomputing anoptimal placement. 6.6. Letusdene amultiplication operation onthree symbolsa;b;caccording tothefollowing table; thusab=b,ba=c,andsoon.Notice that themultiplication operation dened bythetable is neither associative norcommutative .
6âŒ˜24âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 193 abc abba bcba cacc Findanefcient algorithm that examines astring ofthese symbols ,saybbbbac ,and decides whether ornotitispossible toparenthesize thestring insuchawaythat thevalue ofthe resulting expression isa.Forexample ,oninputbbbbac your algorithm should return yesbecause ((b(bb))(ba))c=a. 6.7. Asubsequence ispalindromic ifitisthesame whether read lefttoright orright toleft. For instance ,thesequence A;C;G;T;G;T;C;A;A;A;A;T;C;G hasmany palindromic subsequences ,includingA;C;G;C;AandA;A;A;A(ontheother hand, thesubsequence A;C;Tisnotpalindromic). Devise analgorithm that takes asequencex[1:::n] and returns the(length ofthe) longest palindromic subsequence .Itsrunning time should be O(n2). 6.8. Given twostringsx=x1x2xnandy=y1y2ym,wewish tond thelength oftheir longest common substring ,that is,thelargestkforwhic hthere areindicesiandjwithxixi+1xi+k1= yjyj+1yj+k1.Show how todothisintimeO(mn). 6.9. Acertain string-processing language offers aprimitive operation whic hsplits astring into two pieces .Since this operation involves copying theoriginal string ,ittakesnunits oftime fora string oflengthn,regardless ofthelocation ofthecut. Suppose ,now,that youwanttobreak a string into many pieces .The order inwhic hthebreaks aremade canaffect thetotal running time.Forexample ,ifyouwanttocuta20-character string atpositions 3and10,then making therst cutatposition 3incurs atotal cost of20+17=37,while doing position 10rst hasa better costof20+10=30. Give adynamic programming algorithm that, given thelocations ofmcuts inastring oflength n,nds theminimum costofbreaking thestring intom+1pieces . 6.10. Counting heads .Given integersnandk,along withp1;:::;pn2[0;1],youwanttodetermine the probability ofobtaining exactlykheads whennbiased coins aretossed independently atrandom, wherepiistheprobability that theithcoin comes upheads .Give anO(n2)algorithm forthis task.2Assume youcanmultiply andaddtwonumbers in[0;1]inO(1)time. 6.11. Given twostringsx=x1x2xnandy=y1y2ym,wewish tond thelength oftheir longest common subsequence ,that is,thelargestkforwhic hthere areindicesi1<i2<<ikand j1<j2<<jkwithxi1xi2xik=yj1yj2yjk.Show how todothisintimeO(mn). 6.12. Youaregiven aconvex polygonPonnvertices intheplane (specied bytheirxandycoordi- nates). Atriangulation ofPisacollection ofn3diagonals ofPsuchthat notwodiagonals intersect (except possibly attheir endpoints). Notice that atriangulation splits thepolygon' s interior inton2disjoint triangles .Thecost ofatriangulation isthesum ofthelengths ofthe diagonals init.Give anefcient algorithm fornding atriangulation ofminimum cost. (Hint: Label thevertices ofPby1;:::;n,starting from anarbitrary vertex andwalking clockwise .For 1i<jn,letthesubproblem A(i;j)denote theminimum cost triangulation ofthepolygon spanned byverticesi;i+1;:::;j.) 2Infact, there isalso aO(nlog2n)algorithm within your reach.
6âŒ˜25âŒ˜194 Algorithms 6.13. Consider thefollowing game .AÂ“dealerÂ” produces asequences1snofÂ“cards ,Â”face up,where eachcardsihasavaluevi.Then twoplayers take turns picking acard from thesequence ,but canonly picktherst orthelastcard ofthe(remaining) sequence .Thegoal istocollect cards of largest total value .(Forexample ,youcanthink ofthecards asbills ofdifferent denominations .) Assumeniseven. (a)Show asequence ofcards suchthat itisnotoptimal fortherst playertostart bypicking uptheavailable card oflarger value .That is,thenatural greedy strategy issuboptimal. (b)Give anO(n2)algorithm tocompute anoptimal strategy fortherst player.Given the initial sequence ,your algorithm should precompute inO(n2)time some information, and then therst playershould beable tomake eachmove optimally inO(1)time bylooking uptheprecomputed information. 6.14. Cutting cloth. Youaregiven arectangular piece ofcloth with dimensions XY,whereXand Yarepositive integers ,and alistofnproducts that canbemade using thecloth. Foreach producti2[1;n]youknow that arectangle ofcloth ofdimensions aibiisneeded andthat the nal selling price oftheproduct isci.Assume theai,bi,andciareallpositive integers .You haveamachine that cancutanyrectangular piece ofcloth into twopieces either horizontally or vertically .Design analgorithm that determines thebest return ontheXYpiece ofcloth, that is,astrategy forcutting thecloth sothat theproducts made from theresulting pieces give the maximum sum ofselling prices .Youarefree tomake asmany copies ofagiven product asyou wish, ornone ifdesired. 6.15. Suppose twoteams ,AandB,areplaying amatc htoseewho istherst towinngames (forsome particular n).Wecansuppose thatAandBareequally competent, soeachhasa50% chance ofwinning anyparticular game .Suppose they havealready playedi+jgames ,ofwhic hAhas woniandBhaswonj.Give anefcient algorithm tocompute theprobability thatAwillgoon towinthematc h.Forexample ,ifi=n1andj=n3then theprobability thatAwillwinthe matc his7=8,since itmust winanyofthenext three games . 6.16. Thegarage saleproblem (courtesy ofProfessor Lofti Zadeh). Onagiven Sunda ymorning ,there arengarage sales going on,g1;g2;:::;gn.Foreachgarage salegj,youhaveanestimate ofits value toyou,vj.Foranytwogarage sales youhaveanestimate ofthetransportation costdij ofgetting fromgitogj.Youarealso given thecostsd0janddj0ofgoing between your home andeachgarage sale.Youwanttond atour ofasubset ofthegiven garage sales ,starting and ending athome ,that maximizes your total benet minus your total transportation costs . Give analgorithm that solves thisproblem intimeO(n22n).(Hint: This isclosely related tothe traveling salesman problem.) 6.17. Given anunlimited supply ofcoins ofdenominations x1;x2;:::;xn,wewish tomake change for avaluev;that is,wewish tondasetofcoins whose total value isv.This might notbepossible: forinstance ,ifthedenominations are5and10then wecanmake change for15butnotfor12. Give anO(nv)dynamic-programming algorithm forthefollowing problem. Input:x1;:::;xn;v. Question: Isitpossible tomake change forvusing coins ofdenominations x1;:::;xn? 6.18. Consider thefollowing variation onthechange-making problem (Exercise 6.17): youaregiven denominations x1;x2;:::;xn,andyouwanttomake change foravaluev,butyouareallowed to useeachdenomination atmost once.Forinstance ,ifthedenominations are1;5;10;20,then you canmake change for16=1+15andfor31=1+10+20butnotfor40(because youcan't use20 twice).
6âŒ˜26âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 195 Figure 6.12 Two binary searc htrees forthekeywords ofaprogramming language . end begin else if whiledo thendo thenwhile begin if else end Input: Positive integersx1;x2;:::;xn;another integerv. Output: Can youmake change forv,using eachdenomination xiatmost once? Show how tosolve thisproblem intimeO(nv). 6.19. Here isyetanother variation onthechange-making problem (Exercise 6.17). Given anunlimited supply ofcoins ofdenominations x1;x2;:::;xn,wewish tomake change for avaluevusing atmostkcoins; that is,wewish tond asetofkcoins whose total value isv. This might notbepossible: forinstance ,ifthedenominations are5and10andk=6,then we canmake change for55butnotfor65.Give anefcient dynamic-programming algorithm forthe following problem. Input:x1;:::;xn;k;v. Question: Isitpossible tomake change forvusing atmostkcoins ,ofdenominations x1;:::;xn? 6.20. Optimal binary searc htrees.Suppose weknow thefrequency with whic hkeywords occur in programs ofacertain language ,forinstance: begin 5% do 40% else 8% end 4% if 10% then 10% while 23% Wewanttoorganize them inabinary searc htree,sothat thekeyword intherootisalphabetically bigger than allthekeywords intheleftsubtree andsmaller than allthekeywords intheright subtree (and thisholds forallnodes). Figure 6.12 hasanicely-balanced example ontheleft. Inthis case,when akeyword isbeing looked up,thenumber ofcomparisons needed isatmost three: forinstance ,innding Â“whileÂ”, only thethree nodes Â“endÂ”, Â“thenÂ”, andÂ“whileÂ” getexamined. Butsince weknow thefrequency
6âŒ˜27âŒ˜196 Algorithms with whic hkeywords areaccessed, wecanuseaneven more ne-tuned costfunction, theaverage number ofcomparisons tolook upaword. Forthesearc htree ontheleft, itis cost =1(0:04)+2(0:40+0:10)+3(0:05+0:08+0:10+0:23)=2:42: Bythismeasure ,thebest searc htree istheoneontheright, whic hhasacostof2:18. Give anefcient algorithm forthefollowing task. Input:nwords (insorted order); frequencies ofthese words:p1;p2;:::;pn. Output: Thebinary searc htree oflowest cost (dened above astheexpected number ofcomparisons inlooking upaword). 6.21. Avertex cover ofagraphG=(V;E)isasubset ofverticesSVthat includes atleast one endpoint ofevery edge inE.Give alinear -time algorithm forthefollowing task. Input: Anundirected treeT=(V;E). Output: Thesizeofthesmallest vertex cover ofT. Forinstance ,inthefollowing tree,possible vertex covers includefA;B;C;D;E;F;GgandfA;C;D;Fg butnotfC;E;Fg.Thesmallest vertex cover hassize3:fB;E;Gg. ED A B C FG 6.22. Give anO(nt)algorithm forthefollowing task. Input: Alistofnpositive integersa1;a2;:::;an;apositive integert. Question: Does some subset oftheai'sadduptot?(Youcanuseeachaiatmost once.) (Hint: Look atsubproblems oftheform Â“does asubset offa1;a2;:::;aigadduptos?Â”) 6.23. Amission-critical production system hasnstages that havetobeperformed sequentially; stage iisperformed bymachineMi.EachmachineMihasaprobability rioffunctioning reliably and aprobability 1rioffailing (and thefailures areindependent). Therefore ,ifweimplement eachstage with asingle machine,theprobability that thewhole system works isr1r2rn. Toimprove this probability weadd redundancy ,byhavingmicopies ofthemachineMithat performs stagei.Theprobability that allmicopies failsimultaneously isonly(1ri)mi,sothe probability that stageiiscompleted correctly is1(1ri)miandtheprobability that thewhole system works isQn i=1(1(1ri)mi).EachmachineMihasacostci,andthere isatotal budget Btobuymachines .(Assume thatBandciarepositive integers .) Given theprobabilities r1;:::;rn,thecostsc1;:::;cn,and thebudgetB,nd theredundancies m1;:::;mnthat arewithin theavailable budget and that maximize theprobability that the system works correctly . 6.24. Timeand space complexity ofdynamic programming .Our dynamic programming algorithm for computing theeditdistance between strings oflengthmandncreates atable ofsizenmand therefore needsO(mn)time andspace .Inpractice ,itwillrunoutofspace long before itruns out oftime.How canthisspace requirement bereduced?
6âŒ˜28âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 197 (a)Show that ifwejustwanttocompute thevalue oftheeditdistance (rather than theoptimal sequence ofedits), then onlyO(n)space isneeded, because only asmall portion ofthetable needs tobemaintained atanygiven time. (b)Now suppose that wealso wanttheoptimal sequence ofedits .Aswesawearlier ,this problem canberecast interms ofacorresponding grid-shaped dag,inwhic hthegoal isto ndtheoptimal path from node(0;0)tonode(n;m).Itwillbeconvenient towork with this formulation, andwhile we're talking about convenience ,wemight aswell also assume that misapower of2. Let'sstart with asmall addition totheeditdistance algorithm that willturn outtobevery useful. The optimal path inthedagmust pass through anintermediate node (k;m=2)for somek;show how thealgorithm canbemodied toalso return thisvaluek. (c)Now consider arecursive scheme: procedure find-path ((0;0)!(n;m)) compute thevaluekabove find-path( (0;0)!(k;m=2)) find-path( (k;m=2)!(n;m)) concatenate thesetwopaths,withkinthemiddle Show that thisscheme canbemade toruninO(mn)time andO(n)space . 6.25. Consider thefollowing 3-PARTITIONproblem. Given integersa1;:::;an,wewanttodetermine whether itispossible topartition off1;:::;nginto three disjoint subsetsI;J;Ksuchthat X i2Iai=X j2Jaj=X k2Kak=1 3nX i=1ai Forexample ,forinput (1;2;3;4;4;5;8)theanswer isyes,because there isthepartition (1;8), (4;5),(2;3;4).Ontheother hand, forinput (2;2;3;5)theanswer isno. Devise andanalyze adynamic programming algorithm for3-PARTITIONthat runs intime poly- nomial innandinP iai. 6.26. Sequence alignment. When anew gene isdiscovered, astandard approac htounderstanding its function istolook through adatabase ofknown genes and nd close matc hes.The closeness oftwogenes ismeasured bytheextent towhic hthey arealigned .Toformalize this,think of agene asbeing along string over analphabet =fA;C;G;Tg.Consider twogenes (strings) x=ATGCCandy=TACGCA.Analignment ofxandyisawayofmatc hing upthese two strings bywriting them incolumns ,forinstance: ATGCC TACGCA Here theÂ“Â”indicates aÂ“gap.Â”The characters ofeachstring must appear inorder ,and each column must contain acharacter from atleast oneofthestrings .The score ofanalignment is specied byascoring matrixofsize(jj+1)(jj+1),where theextra rowandcolumn areto accommodate gaps .Forinstance thepreceding alignment hasthefollowing score: (;T)+(A;A)+(T;)+(;C)+(G;G)+(C;C)+(C;A): Give adynamic programming algorithm that takes asinput twostringsx[1:::n]andy[1:::m] andascoring matrix,andreturns thehighest-scoring alignment. Therunning time should be O(mn).
6âŒ˜29âŒ˜198 Algorithms 6.27. Alignment with gappenalties .The alignment algorithm ofExercise 6.26 helps toidentify DNA sequences that areclose tooneanother .The discrepancies between these closely matc hedse- quences areoften caused byerrors inDNA replication. However ,acloser look atthebiological replication process reveals that thescoring function weconsidered earlier hasaqualitative prob- lem: nature often inserts orremoves entire substrings ofnucleotides (creating long gaps), rather than editing justoneposition atatime.Therefore ,thepenalty foragapoflength 10should not be10times thepenalty foragapoflength 1,butsomething signicantly smaller . Repeat Exercise 6.26, butthis time useamodied scoring function inwhic hthepenalty fora gapoflengthkisc0+c1k,wherec0andc1aregiven constants (andc0islarger thanc1). 6.28. Local sequence alignment. Often twoDNA sequences aresignicantly different, butcontain re- gions that arevery similar andarehighl yconserved .Design analgorithm that takes aninput twostringsx[1:::n]andy[1:::m]andascoring matrix(asdened inExercise 6.26), andout- puts substrings x0andy0ofxandy,respectively ,that havethehighest-scoring alignment over allpairs ofsuchsubstrings .Youralgorithm should take timeO(mn). 6.29. Exon chaining .Eachgene corresponds toasubregion oftheoverall genome (theDNA sequence); however ,part ofthis region might beÂ“junk DNA. Â”Frequently ,agene consists ofseveral pieces called exons ,whic hareseparated byjunk fragments called introns .This complicates theprocess ofidentifying genes inanewly sequenced genome . Suppose wehaveanew DNA sequence andwewanttocheckwhether acertain gene (astring) is present init.Because wecannot hope that thegene willbeacontiguous subsequence ,welook for partial matc hesÂ—fragments oftheDNA that arealso present inthegene (actually ,even these partial matc heswillbeapproximate ,notperfect). Wethen attempt toassemble these fragments . Letx[1:::n]denote theDNA sequence .Eachpartial matc hcanberepresented byatriple (li;ri;wi),wherex[li:::ri]isthefragment andwiisaweight representing thestrength ofthe matc h(itmight bealocal alignment score orsome other statistical quantity). Many ofthese potential matc hescould befalse ,sothegoal istond asubset ofthetriples that areconsistent (nonoverlapping) andhaveamaximum total weight. Show how todothisefciently . 6.30. Reconstructing evolutionary trees bymaximum parsimony .Suppose wemanage tosequence a particular gene across awhole bunc hofdifferent species .Forconcreteness ,saythere aren species ,andthesequences arestrings oflengthkover alphabet =fA;C;G;Tg.How canwe usethisinformation toreconstruct theevolutionary history ofthese species? Evolutionary history iscommonly represented byatree whose leavesarethedifferent species , whose root istheir common ancestor ,andwhose internal branc hesrepresent speciation events (that is,moments when anew species broke offfrom anexisting one). Thus weneed tondthe following: Anevolutionary tree with thegiven species attheleaves. Foreachinternal node ,astring oflengthk:thegene sequence forthat particular ancestor . Foreachpossible treeT,annotated with sequencess(u)2kateachofitsnodesu,wecanassign ascore based ontheprinciple ofparsimony :fewer mutations aremore likely . score(T)=X (u;v)2E(T)(number ofpositions onwhic hs(u)ands(v)disagree): Finding thehighest-score tree isadifcult problem. Here wewillconsider justasmall part of it:suppose weknow thestructure ofthetree,and wewanttollinthesequences s(u)ofthe internal nodesu.Here' sanexample withk=4andn=5:
6âŒ˜30âŒ˜S.Dasgupta, C.H.Papadimitriou, andU.V.Vazirani 199 CGCG AGGA ATCA AGTC ATTC (a)Inthis particular example ,there areseveral maximum parsimony reconstructions ofthe internal node sequences .Findoneofthem. (b)Give anefcient (interms ofnandk)algorithm forthis task. (Hint: Even though the sequences might belong,youcandojustoneposition atatime.)
